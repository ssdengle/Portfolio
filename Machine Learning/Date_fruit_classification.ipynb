{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq1oeum5uKA-"
      },
      "source": [
        "# Date Fruit Classification.\n",
        "Srinivas Dengle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem Statement:\n",
        "In food production it is important to properly label ingredients for both health and business reasons. However, sometimes mistakes are made and there is room for improvement in food labeling practices. A number of different types of dates are grown around the world, and it takes expertise to correctly identify the variety. Your job as a machine learning developer is to create a model that can identify the type of date from external features such as colour, length, diameter and shape factors which have been determined by a computer vision model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6PV-f5kuJ1i"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-C0nAp0cye5z"
      },
      "outputs": [],
      "source": [
        "# Importing Libararies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KckL4R84whby"
      },
      "source": [
        "# 1A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "22OeAwOEye50",
        "outputId": "030d7ef7-9786-487f-ba05-b0e9a64ec2de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AREA</th>\n",
              "      <th>PERIMETER</th>\n",
              "      <th>MAJOR_AXIS</th>\n",
              "      <th>MINOR_AXIS</th>\n",
              "      <th>ECCENTRICITY</th>\n",
              "      <th>EQDIASQ</th>\n",
              "      <th>SOLIDITY</th>\n",
              "      <th>CONVEX_AREA</th>\n",
              "      <th>EXTENT</th>\n",
              "      <th>ASPECT_RATIO</th>\n",
              "      <th>...</th>\n",
              "      <th>KurtosisRR</th>\n",
              "      <th>KurtosisRG</th>\n",
              "      <th>KurtosisRB</th>\n",
              "      <th>EntropyRR</th>\n",
              "      <th>EntropyRG</th>\n",
              "      <th>EntropyRB</th>\n",
              "      <th>ALLdaub4RR</th>\n",
              "      <th>ALLdaub4RG</th>\n",
              "      <th>ALLdaub4RB</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>422163</td>\n",
              "      <td>2378.9080</td>\n",
              "      <td>837.8484</td>\n",
              "      <td>645.6693</td>\n",
              "      <td>0.6373</td>\n",
              "      <td>733.1539</td>\n",
              "      <td>0.9947</td>\n",
              "      <td>424428</td>\n",
              "      <td>0.7831</td>\n",
              "      <td>1.2976</td>\n",
              "      <td>...</td>\n",
              "      <td>3.2370</td>\n",
              "      <td>2.9574</td>\n",
              "      <td>4.2287</td>\n",
              "      <td>-5.919126e+10</td>\n",
              "      <td>-50714214400</td>\n",
              "      <td>-39922372608</td>\n",
              "      <td>58.7255</td>\n",
              "      <td>54.9554</td>\n",
              "      <td>47.8400</td>\n",
              "      <td>BERHI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>338136</td>\n",
              "      <td>2085.1440</td>\n",
              "      <td>723.8198</td>\n",
              "      <td>595.2073</td>\n",
              "      <td>0.5690</td>\n",
              "      <td>656.1464</td>\n",
              "      <td>0.9974</td>\n",
              "      <td>339014</td>\n",
              "      <td>0.7795</td>\n",
              "      <td>1.2161</td>\n",
              "      <td>...</td>\n",
              "      <td>2.6228</td>\n",
              "      <td>2.6350</td>\n",
              "      <td>3.1704</td>\n",
              "      <td>-3.423307e+10</td>\n",
              "      <td>-37462601728</td>\n",
              "      <td>-31477794816</td>\n",
              "      <td>50.0259</td>\n",
              "      <td>52.8168</td>\n",
              "      <td>47.8315</td>\n",
              "      <td>BERHI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>526843</td>\n",
              "      <td>2647.3940</td>\n",
              "      <td>940.7379</td>\n",
              "      <td>715.3638</td>\n",
              "      <td>0.6494</td>\n",
              "      <td>819.0222</td>\n",
              "      <td>0.9962</td>\n",
              "      <td>528876</td>\n",
              "      <td>0.7657</td>\n",
              "      <td>1.3150</td>\n",
              "      <td>...</td>\n",
              "      <td>3.7516</td>\n",
              "      <td>3.8611</td>\n",
              "      <td>4.7192</td>\n",
              "      <td>-9.394835e+10</td>\n",
              "      <td>-74738221056</td>\n",
              "      <td>-60311207936</td>\n",
              "      <td>65.4772</td>\n",
              "      <td>59.2860</td>\n",
              "      <td>51.9378</td>\n",
              "      <td>BERHI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>416063</td>\n",
              "      <td>2351.2100</td>\n",
              "      <td>827.9804</td>\n",
              "      <td>645.2988</td>\n",
              "      <td>0.6266</td>\n",
              "      <td>727.8378</td>\n",
              "      <td>0.9948</td>\n",
              "      <td>418255</td>\n",
              "      <td>0.7759</td>\n",
              "      <td>1.2831</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0401</td>\n",
              "      <td>8.6136</td>\n",
              "      <td>8.2618</td>\n",
              "      <td>-3.207431e+10</td>\n",
              "      <td>-32060925952</td>\n",
              "      <td>-29575010304</td>\n",
              "      <td>43.3900</td>\n",
              "      <td>44.1259</td>\n",
              "      <td>41.1882</td>\n",
              "      <td>BERHI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>347562</td>\n",
              "      <td>2160.3540</td>\n",
              "      <td>763.9877</td>\n",
              "      <td>582.8359</td>\n",
              "      <td>0.6465</td>\n",
              "      <td>665.2291</td>\n",
              "      <td>0.9908</td>\n",
              "      <td>350797</td>\n",
              "      <td>0.7569</td>\n",
              "      <td>1.3108</td>\n",
              "      <td>...</td>\n",
              "      <td>2.7016</td>\n",
              "      <td>2.9761</td>\n",
              "      <td>4.4146</td>\n",
              "      <td>-3.998097e+10</td>\n",
              "      <td>-35980042240</td>\n",
              "      <td>-25593278464</td>\n",
              "      <td>52.7743</td>\n",
              "      <td>50.9080</td>\n",
              "      <td>42.6666</td>\n",
              "      <td>BERHI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893</th>\n",
              "      <td>255403</td>\n",
              "      <td>1925.3650</td>\n",
              "      <td>691.8453</td>\n",
              "      <td>477.1796</td>\n",
              "      <td>0.7241</td>\n",
              "      <td>570.2536</td>\n",
              "      <td>0.9785</td>\n",
              "      <td>261028</td>\n",
              "      <td>0.7269</td>\n",
              "      <td>1.4499</td>\n",
              "      <td>...</td>\n",
              "      <td>2.2423</td>\n",
              "      <td>2.3704</td>\n",
              "      <td>2.7202</td>\n",
              "      <td>-2.529642e+10</td>\n",
              "      <td>-19168882688</td>\n",
              "      <td>-18473392128</td>\n",
              "      <td>49.0869</td>\n",
              "      <td>43.0422</td>\n",
              "      <td>42.4153</td>\n",
              "      <td>SOGAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>894</th>\n",
              "      <td>365924</td>\n",
              "      <td>2664.8230</td>\n",
              "      <td>855.4633</td>\n",
              "      <td>551.5447</td>\n",
              "      <td>0.7644</td>\n",
              "      <td>682.5752</td>\n",
              "      <td>0.9466</td>\n",
              "      <td>386566</td>\n",
              "      <td>0.6695</td>\n",
              "      <td>1.5510</td>\n",
              "      <td>...</td>\n",
              "      <td>3.4109</td>\n",
              "      <td>3.5805</td>\n",
              "      <td>3.9910</td>\n",
              "      <td>-3.160522e+10</td>\n",
              "      <td>-21945366528</td>\n",
              "      <td>-19277905920</td>\n",
              "      <td>46.8086</td>\n",
              "      <td>39.1046</td>\n",
              "      <td>36.5502</td>\n",
              "      <td>SOGAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>254330</td>\n",
              "      <td>1926.7360</td>\n",
              "      <td>747.4943</td>\n",
              "      <td>435.6219</td>\n",
              "      <td>0.8126</td>\n",
              "      <td>569.0545</td>\n",
              "      <td>0.9925</td>\n",
              "      <td>256255</td>\n",
              "      <td>0.7240</td>\n",
              "      <td>1.7159</td>\n",
              "      <td>...</td>\n",
              "      <td>2.2759</td>\n",
              "      <td>2.5090</td>\n",
              "      <td>2.6951</td>\n",
              "      <td>-2.224277e+10</td>\n",
              "      <td>-19594921984</td>\n",
              "      <td>-17592152064</td>\n",
              "      <td>44.1325</td>\n",
              "      <td>40.7986</td>\n",
              "      <td>40.9769</td>\n",
              "      <td>SOGAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>238955</td>\n",
              "      <td>1906.2679</td>\n",
              "      <td>716.6485</td>\n",
              "      <td>441.8297</td>\n",
              "      <td>0.7873</td>\n",
              "      <td>551.5859</td>\n",
              "      <td>0.9604</td>\n",
              "      <td>248795</td>\n",
              "      <td>0.6954</td>\n",
              "      <td>1.6220</td>\n",
              "      <td>...</td>\n",
              "      <td>2.6769</td>\n",
              "      <td>2.6874</td>\n",
              "      <td>2.7991</td>\n",
              "      <td>-2.604860e+10</td>\n",
              "      <td>-21299822592</td>\n",
              "      <td>-19809978368</td>\n",
              "      <td>51.2267</td>\n",
              "      <td>45.7162</td>\n",
              "      <td>45.6260</td>\n",
              "      <td>SOGAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>343792</td>\n",
              "      <td>2289.2720</td>\n",
              "      <td>823.8438</td>\n",
              "      <td>534.7757</td>\n",
              "      <td>0.7607</td>\n",
              "      <td>661.6113</td>\n",
              "      <td>0.9781</td>\n",
              "      <td>351472</td>\n",
              "      <td>0.6941</td>\n",
              "      <td>1.5405</td>\n",
              "      <td>...</td>\n",
              "      <td>2.5138</td>\n",
              "      <td>3.0369</td>\n",
              "      <td>3.0865</td>\n",
              "      <td>-3.198348e+10</td>\n",
              "      <td>-20482514944</td>\n",
              "      <td>-21219354624</td>\n",
              "      <td>47.3454</td>\n",
              "      <td>38.6966</td>\n",
              "      <td>39.6738</td>\n",
              "      <td>SOGAY</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>898 rows × 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  \\\n",
              "0    422163  2378.9080    837.8484    645.6693        0.6373  733.1539   \n",
              "1    338136  2085.1440    723.8198    595.2073        0.5690  656.1464   \n",
              "2    526843  2647.3940    940.7379    715.3638        0.6494  819.0222   \n",
              "3    416063  2351.2100    827.9804    645.2988        0.6266  727.8378   \n",
              "4    347562  2160.3540    763.9877    582.8359        0.6465  665.2291   \n",
              "..      ...        ...         ...         ...           ...       ...   \n",
              "893  255403  1925.3650    691.8453    477.1796        0.7241  570.2536   \n",
              "894  365924  2664.8230    855.4633    551.5447        0.7644  682.5752   \n",
              "895  254330  1926.7360    747.4943    435.6219        0.8126  569.0545   \n",
              "896  238955  1906.2679    716.6485    441.8297        0.7873  551.5859   \n",
              "897  343792  2289.2720    823.8438    534.7757        0.7607  661.6113   \n",
              "\n",
              "     SOLIDITY  CONVEX_AREA  EXTENT  ASPECT_RATIO  ...  KurtosisRR  KurtosisRG  \\\n",
              "0      0.9947       424428  0.7831        1.2976  ...      3.2370      2.9574   \n",
              "1      0.9974       339014  0.7795        1.2161  ...      2.6228      2.6350   \n",
              "2      0.9962       528876  0.7657        1.3150  ...      3.7516      3.8611   \n",
              "3      0.9948       418255  0.7759        1.2831  ...      5.0401      8.6136   \n",
              "4      0.9908       350797  0.7569        1.3108  ...      2.7016      2.9761   \n",
              "..        ...          ...     ...           ...  ...         ...         ...   \n",
              "893    0.9785       261028  0.7269        1.4499  ...      2.2423      2.3704   \n",
              "894    0.9466       386566  0.6695        1.5510  ...      3.4109      3.5805   \n",
              "895    0.9925       256255  0.7240        1.7159  ...      2.2759      2.5090   \n",
              "896    0.9604       248795  0.6954        1.6220  ...      2.6769      2.6874   \n",
              "897    0.9781       351472  0.6941        1.5405  ...      2.5138      3.0369   \n",
              "\n",
              "     KurtosisRB     EntropyRR    EntropyRG    EntropyRB  ALLdaub4RR  \\\n",
              "0        4.2287 -5.919126e+10 -50714214400 -39922372608     58.7255   \n",
              "1        3.1704 -3.423307e+10 -37462601728 -31477794816     50.0259   \n",
              "2        4.7192 -9.394835e+10 -74738221056 -60311207936     65.4772   \n",
              "3        8.2618 -3.207431e+10 -32060925952 -29575010304     43.3900   \n",
              "4        4.4146 -3.998097e+10 -35980042240 -25593278464     52.7743   \n",
              "..          ...           ...          ...          ...         ...   \n",
              "893      2.7202 -2.529642e+10 -19168882688 -18473392128     49.0869   \n",
              "894      3.9910 -3.160522e+10 -21945366528 -19277905920     46.8086   \n",
              "895      2.6951 -2.224277e+10 -19594921984 -17592152064     44.1325   \n",
              "896      2.7991 -2.604860e+10 -21299822592 -19809978368     51.2267   \n",
              "897      3.0865 -3.198348e+10 -20482514944 -21219354624     47.3454   \n",
              "\n",
              "     ALLdaub4RG  ALLdaub4RB  Class  \n",
              "0       54.9554     47.8400  BERHI  \n",
              "1       52.8168     47.8315  BERHI  \n",
              "2       59.2860     51.9378  BERHI  \n",
              "3       44.1259     41.1882  BERHI  \n",
              "4       50.9080     42.6666  BERHI  \n",
              "..          ...         ...    ...  \n",
              "893     43.0422     42.4153  SOGAY  \n",
              "894     39.1046     36.5502  SOGAY  \n",
              "895     40.7986     40.9769  SOGAY  \n",
              "896     45.7162     45.6260  SOGAY  \n",
              "897     38.6966     39.6738  SOGAY  \n",
              "\n",
              "[898 rows x 35 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load Datasets\n",
        "cd = pd.read_csv(\"Date_Fruit_Datasets.csv\")\n",
        "cd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "cJuqyNuDye51",
        "outputId": "548296f6-d4b8-44e5-a196-6eb6e27f2b69"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AREA</th>\n",
              "      <th>PERIMETER</th>\n",
              "      <th>MAJOR_AXIS</th>\n",
              "      <th>MINOR_AXIS</th>\n",
              "      <th>ECCENTRICITY</th>\n",
              "      <th>EQDIASQ</th>\n",
              "      <th>SOLIDITY</th>\n",
              "      <th>CONVEX_AREA</th>\n",
              "      <th>EXTENT</th>\n",
              "      <th>ASPECT_RATIO</th>\n",
              "      <th>ROUNDNESS</th>\n",
              "      <th>COMPACTNESS</th>\n",
              "      <th>SHAPEFACTOR_1</th>\n",
              "      <th>SHAPEFACTOR_2</th>\n",
              "      <th>SHAPEFACTOR_3</th>\n",
              "      <th>SHAPEFACTOR_4</th>\n",
              "      <th>MeanRR</th>\n",
              "      <th>MeanRG</th>\n",
              "      <th>MeanRB</th>\n",
              "      <th>StdDevRR</th>\n",
              "      <th>StdDevRG</th>\n",
              "      <th>StdDevRB</th>\n",
              "      <th>SkewRR</th>\n",
              "      <th>SkewRG</th>\n",
              "      <th>SkewRB</th>\n",
              "      <th>KurtosisRR</th>\n",
              "      <th>KurtosisRG</th>\n",
              "      <th>KurtosisRB</th>\n",
              "      <th>EntropyRR</th>\n",
              "      <th>EntropyRG</th>\n",
              "      <th>EntropyRB</th>\n",
              "      <th>ALLdaub4RR</th>\n",
              "      <th>ALLdaub4RG</th>\n",
              "      <th>ALLdaub4RB</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>8.980000e+02</td>\n",
              "      <td>8.980000e+02</td>\n",
              "      <td>8.980000e+02</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "      <td>898.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>298295.207127</td>\n",
              "      <td>2057.660953</td>\n",
              "      <td>750.811994</td>\n",
              "      <td>495.872785</td>\n",
              "      <td>0.737468</td>\n",
              "      <td>604.577938</td>\n",
              "      <td>0.981840</td>\n",
              "      <td>303845.592428</td>\n",
              "      <td>0.736267</td>\n",
              "      <td>2.131102</td>\n",
              "      <td>0.857720</td>\n",
              "      <td>0.807190</td>\n",
              "      <td>0.003428</td>\n",
              "      <td>0.001794</td>\n",
              "      <td>0.655420</td>\n",
              "      <td>0.988680</td>\n",
              "      <td>100.165885</td>\n",
              "      <td>97.609401</td>\n",
              "      <td>96.194889</td>\n",
              "      <td>29.047436</td>\n",
              "      <td>26.383362</td>\n",
              "      <td>26.148330</td>\n",
              "      <td>0.089266</td>\n",
              "      <td>0.564139</td>\n",
              "      <td>0.250518</td>\n",
              "      <td>4.247845</td>\n",
              "      <td>5.110894</td>\n",
              "      <td>3.780928</td>\n",
              "      <td>-3.185021e+10</td>\n",
              "      <td>-2.901860e+10</td>\n",
              "      <td>-2.771876e+10</td>\n",
              "      <td>50.082888</td>\n",
              "      <td>48.805681</td>\n",
              "      <td>48.098393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>107245.205337</td>\n",
              "      <td>410.012459</td>\n",
              "      <td>144.059326</td>\n",
              "      <td>114.268917</td>\n",
              "      <td>0.088727</td>\n",
              "      <td>119.593888</td>\n",
              "      <td>0.018157</td>\n",
              "      <td>108815.656947</td>\n",
              "      <td>0.053745</td>\n",
              "      <td>17.820778</td>\n",
              "      <td>0.070839</td>\n",
              "      <td>0.062175</td>\n",
              "      <td>0.020456</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>0.094314</td>\n",
              "      <td>0.020473</td>\n",
              "      <td>32.126549</td>\n",
              "      <td>28.251801</td>\n",
              "      <td>21.627409</td>\n",
              "      <td>6.306931</td>\n",
              "      <td>5.462164</td>\n",
              "      <td>4.905078</td>\n",
              "      <td>0.943285</td>\n",
              "      <td>1.039813</td>\n",
              "      <td>0.632918</td>\n",
              "      <td>2.892357</td>\n",
              "      <td>3.745463</td>\n",
              "      <td>2.049831</td>\n",
              "      <td>2.037241e+10</td>\n",
              "      <td>1.712952e+10</td>\n",
              "      <td>1.484137e+10</td>\n",
              "      <td>16.063125</td>\n",
              "      <td>14.125911</td>\n",
              "      <td>10.813862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1987.000000</td>\n",
              "      <td>911.828000</td>\n",
              "      <td>336.722700</td>\n",
              "      <td>2.283200</td>\n",
              "      <td>0.344800</td>\n",
              "      <td>50.298400</td>\n",
              "      <td>0.836600</td>\n",
              "      <td>2257.000000</td>\n",
              "      <td>0.512300</td>\n",
              "      <td>1.065300</td>\n",
              "      <td>0.004800</td>\n",
              "      <td>0.041100</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.756800</td>\n",
              "      <td>30.382600</td>\n",
              "      <td>41.048000</td>\n",
              "      <td>44.256900</td>\n",
              "      <td>7.303800</td>\n",
              "      <td>8.655700</td>\n",
              "      <td>5.148600</td>\n",
              "      <td>-1.724200</td>\n",
              "      <td>-1.834400</td>\n",
              "      <td>-1.029100</td>\n",
              "      <td>1.708200</td>\n",
              "      <td>1.607600</td>\n",
              "      <td>1.767200</td>\n",
              "      <td>-1.091220e+11</td>\n",
              "      <td>-9.261697e+10</td>\n",
              "      <td>-8.747177e+10</td>\n",
              "      <td>15.191100</td>\n",
              "      <td>20.524700</td>\n",
              "      <td>22.130000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>206948.000000</td>\n",
              "      <td>1726.091500</td>\n",
              "      <td>641.068650</td>\n",
              "      <td>404.684375</td>\n",
              "      <td>0.685625</td>\n",
              "      <td>513.317075</td>\n",
              "      <td>0.978825</td>\n",
              "      <td>210022.750000</td>\n",
              "      <td>0.705875</td>\n",
              "      <td>1.373725</td>\n",
              "      <td>0.827750</td>\n",
              "      <td>0.768050</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.589975</td>\n",
              "      <td>0.989300</td>\n",
              "      <td>76.448000</td>\n",
              "      <td>77.306125</td>\n",
              "      <td>78.502375</td>\n",
              "      <td>24.901525</td>\n",
              "      <td>22.289650</td>\n",
              "      <td>22.890975</td>\n",
              "      <td>-0.549900</td>\n",
              "      <td>-0.230200</td>\n",
              "      <td>-0.196950</td>\n",
              "      <td>2.536625</td>\n",
              "      <td>2.508850</td>\n",
              "      <td>2.577275</td>\n",
              "      <td>-4.429444e+10</td>\n",
              "      <td>-3.894638e+10</td>\n",
              "      <td>-3.564534e+10</td>\n",
              "      <td>38.224425</td>\n",
              "      <td>38.654525</td>\n",
              "      <td>39.250725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>319833.000000</td>\n",
              "      <td>2196.345450</td>\n",
              "      <td>791.363400</td>\n",
              "      <td>495.054850</td>\n",
              "      <td>0.754700</td>\n",
              "      <td>638.140950</td>\n",
              "      <td>0.987300</td>\n",
              "      <td>327207.000000</td>\n",
              "      <td>0.746950</td>\n",
              "      <td>1.524150</td>\n",
              "      <td>0.867750</td>\n",
              "      <td>0.804950</td>\n",
              "      <td>0.002600</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>0.647950</td>\n",
              "      <td>0.993600</td>\n",
              "      <td>107.682450</td>\n",
              "      <td>100.676000</td>\n",
              "      <td>99.225600</td>\n",
              "      <td>29.709450</td>\n",
              "      <td>25.638300</td>\n",
              "      <td>26.469000</td>\n",
              "      <td>-0.162700</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.135550</td>\n",
              "      <td>3.069800</td>\n",
              "      <td>3.127800</td>\n",
              "      <td>3.080700</td>\n",
              "      <td>-2.826156e+10</td>\n",
              "      <td>-2.620990e+10</td>\n",
              "      <td>-2.392928e+10</td>\n",
              "      <td>53.841300</td>\n",
              "      <td>50.337800</td>\n",
              "      <td>49.614100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>382573.000000</td>\n",
              "      <td>2389.716575</td>\n",
              "      <td>858.633750</td>\n",
              "      <td>589.031700</td>\n",
              "      <td>0.802150</td>\n",
              "      <td>697.930525</td>\n",
              "      <td>0.991800</td>\n",
              "      <td>388804.000000</td>\n",
              "      <td>0.775850</td>\n",
              "      <td>1.674750</td>\n",
              "      <td>0.899500</td>\n",
              "      <td>0.848875</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>0.002075</td>\n",
              "      <td>0.720625</td>\n",
              "      <td>0.996400</td>\n",
              "      <td>126.127450</td>\n",
              "      <td>119.159600</td>\n",
              "      <td>113.332250</td>\n",
              "      <td>33.274375</td>\n",
              "      <td>29.905325</td>\n",
              "      <td>29.482375</td>\n",
              "      <td>0.471025</td>\n",
              "      <td>1.406550</td>\n",
              "      <td>0.593950</td>\n",
              "      <td>4.449850</td>\n",
              "      <td>7.320400</td>\n",
              "      <td>4.283125</td>\n",
              "      <td>-1.460482e+10</td>\n",
              "      <td>-1.433105e+10</td>\n",
              "      <td>-1.660367e+10</td>\n",
              "      <td>63.063350</td>\n",
              "      <td>59.573600</td>\n",
              "      <td>56.666675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>546063.000000</td>\n",
              "      <td>2811.997100</td>\n",
              "      <td>1222.723000</td>\n",
              "      <td>766.453600</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>833.827900</td>\n",
              "      <td>0.997400</td>\n",
              "      <td>552598.000000</td>\n",
              "      <td>0.856200</td>\n",
              "      <td>535.525700</td>\n",
              "      <td>0.977300</td>\n",
              "      <td>0.968100</td>\n",
              "      <td>0.615400</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.937300</td>\n",
              "      <td>0.999500</td>\n",
              "      <td>159.649400</td>\n",
              "      <td>166.135400</td>\n",
              "      <td>148.211400</td>\n",
              "      <td>48.571200</td>\n",
              "      <td>45.510700</td>\n",
              "      <td>42.422800</td>\n",
              "      <td>3.223600</td>\n",
              "      <td>3.697100</td>\n",
              "      <td>3.092300</td>\n",
              "      <td>26.171100</td>\n",
              "      <td>26.736700</td>\n",
              "      <td>32.249500</td>\n",
              "      <td>-1.627316e+08</td>\n",
              "      <td>-5.627727e+08</td>\n",
              "      <td>-4.370435e+08</td>\n",
              "      <td>79.828900</td>\n",
              "      <td>83.064900</td>\n",
              "      <td>74.104600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                AREA    PERIMETER   MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY  \\\n",
              "count     898.000000   898.000000   898.000000  898.000000    898.000000   \n",
              "mean   298295.207127  2057.660953   750.811994  495.872785      0.737468   \n",
              "std    107245.205337   410.012459   144.059326  114.268917      0.088727   \n",
              "min      1987.000000   911.828000   336.722700    2.283200      0.344800   \n",
              "25%    206948.000000  1726.091500   641.068650  404.684375      0.685625   \n",
              "50%    319833.000000  2196.345450   791.363400  495.054850      0.754700   \n",
              "75%    382573.000000  2389.716575   858.633750  589.031700      0.802150   \n",
              "max    546063.000000  2811.997100  1222.723000  766.453600      1.000000   \n",
              "\n",
              "          EQDIASQ    SOLIDITY    CONVEX_AREA      EXTENT  ASPECT_RATIO  \\\n",
              "count  898.000000  898.000000     898.000000  898.000000    898.000000   \n",
              "mean   604.577938    0.981840  303845.592428    0.736267      2.131102   \n",
              "std    119.593888    0.018157  108815.656947    0.053745     17.820778   \n",
              "min     50.298400    0.836600    2257.000000    0.512300      1.065300   \n",
              "25%    513.317075    0.978825  210022.750000    0.705875      1.373725   \n",
              "50%    638.140950    0.987300  327207.000000    0.746950      1.524150   \n",
              "75%    697.930525    0.991800  388804.000000    0.775850      1.674750   \n",
              "max    833.827900    0.997400  552598.000000    0.856200    535.525700   \n",
              "\n",
              "        ROUNDNESS  COMPACTNESS  SHAPEFACTOR_1  SHAPEFACTOR_2  SHAPEFACTOR_3  \\\n",
              "count  898.000000   898.000000     898.000000     898.000000     898.000000   \n",
              "mean     0.857720     0.807190       0.003428       0.001794       0.655420   \n",
              "std      0.070839     0.062175       0.020456       0.000428       0.094314   \n",
              "min      0.004800     0.041100       0.001700       0.001100       0.001700   \n",
              "25%      0.827750     0.768050       0.002200       0.001500       0.589975   \n",
              "50%      0.867750     0.804950       0.002600       0.001600       0.647950   \n",
              "75%      0.899500     0.848875       0.003200       0.002075       0.720625   \n",
              "max      0.977300     0.968100       0.615400       0.004300       0.937300   \n",
              "\n",
              "       SHAPEFACTOR_4      MeanRR      MeanRG      MeanRB    StdDevRR  \\\n",
              "count     898.000000  898.000000  898.000000  898.000000  898.000000   \n",
              "mean        0.988680  100.165885   97.609401   96.194889   29.047436   \n",
              "std         0.020473   32.126549   28.251801   21.627409    6.306931   \n",
              "min         0.756800   30.382600   41.048000   44.256900    7.303800   \n",
              "25%         0.989300   76.448000   77.306125   78.502375   24.901525   \n",
              "50%         0.993600  107.682450  100.676000   99.225600   29.709450   \n",
              "75%         0.996400  126.127450  119.159600  113.332250   33.274375   \n",
              "max         0.999500  159.649400  166.135400  148.211400   48.571200   \n",
              "\n",
              "         StdDevRG    StdDevRB      SkewRR      SkewRG      SkewRB  KurtosisRR  \\\n",
              "count  898.000000  898.000000  898.000000  898.000000  898.000000  898.000000   \n",
              "mean    26.383362   26.148330    0.089266    0.564139    0.250518    4.247845   \n",
              "std      5.462164    4.905078    0.943285    1.039813    0.632918    2.892357   \n",
              "min      8.655700    5.148600   -1.724200   -1.834400   -1.029100    1.708200   \n",
              "25%     22.289650   22.890975   -0.549900   -0.230200   -0.196950    2.536625   \n",
              "50%     25.638300   26.469000   -0.162700    0.243750    0.135550    3.069800   \n",
              "75%     29.905325   29.482375    0.471025    1.406550    0.593950    4.449850   \n",
              "max     45.510700   42.422800    3.223600    3.697100    3.092300   26.171100   \n",
              "\n",
              "       KurtosisRG  KurtosisRB     EntropyRR     EntropyRG     EntropyRB  \\\n",
              "count  898.000000  898.000000  8.980000e+02  8.980000e+02  8.980000e+02   \n",
              "mean     5.110894    3.780928 -3.185021e+10 -2.901860e+10 -2.771876e+10   \n",
              "std      3.745463    2.049831  2.037241e+10  1.712952e+10  1.484137e+10   \n",
              "min      1.607600    1.767200 -1.091220e+11 -9.261697e+10 -8.747177e+10   \n",
              "25%      2.508850    2.577275 -4.429444e+10 -3.894638e+10 -3.564534e+10   \n",
              "50%      3.127800    3.080700 -2.826156e+10 -2.620990e+10 -2.392928e+10   \n",
              "75%      7.320400    4.283125 -1.460482e+10 -1.433105e+10 -1.660367e+10   \n",
              "max     26.736700   32.249500 -1.627316e+08 -5.627727e+08 -4.370435e+08   \n",
              "\n",
              "       ALLdaub4RR  ALLdaub4RG  ALLdaub4RB  \n",
              "count  898.000000  898.000000  898.000000  \n",
              "mean    50.082888   48.805681   48.098393  \n",
              "std     16.063125   14.125911   10.813862  \n",
              "min     15.191100   20.524700   22.130000  \n",
              "25%     38.224425   38.654525   39.250725  \n",
              "50%     53.841300   50.337800   49.614100  \n",
              "75%     63.063350   59.573600   56.666675  \n",
              "max     79.828900   83.064900   74.104600  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# describe the dataset\n",
        "pd.set_option('display.max_columns', 40)\n",
        "cd.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_eBFA3kw00y"
      },
      "source": [
        "In this dataset we have 34 features(columns) and 898 entities. our label would be Class.\n",
        "\n",
        "# Morphological features\n",
        "\n",
        "1. **AREA**: surface area of the date.\n",
        "2. **PERIMETER**: The perimeter length of the date.\n",
        "3. **MAJOR_AXIS**: The length of longest axis of the date.\n",
        "4. **MINOR_AXIS**: The length of the minor shortest axis of the date.\n",
        "5. **ECCENTRICITY**: The eccentricity\n",
        "of a date\n",
        "6. **EQDIASQ**:Equivalent diameter.\n",
        "7. **SOLIDITY**: It considered the convex and convex condition of the date\n",
        "8. **CONVEX_AREA**: It gave the number of pixels of the smallest convex shell at the region formed by a date.\n",
        "9. **EXTENT**: It returned the ratio of a date fruit's area to the area of the bounding box that encompasses it.\n",
        "10. **ASPECT_RATIO**: Describes the aspect ratio of the date.\n",
        "11. **ROUNDNESS**: Indicates how round the date is.\n",
        "12. **COMPACTNESS**: A metric for the date's compactness.\n",
        "\n",
        "# Shape features\n",
        "13. **SHAPEFACTOR_1 to SHAPEFACTOR_4**: Metrics that describe the shape of the date in various ways.\n",
        "\n",
        "# Color features\n",
        "14. **MeanRR, MeanRG, MeanRB**: Could indicate color characteristics of the date, particularly in the Red spectrum.\n",
        "15. **StdDevRR, StdDevRG, StdDevRB**: Variability in the color features of the date.\n",
        "16. **SkewRR, SkewRG, SkewRB**: Asymmetry in the color distribution of the date.\n",
        "17. **KurtosisRR, KurtosisRG, KurtosisRB**: How peaked or flat the color distribution is for the date.\n",
        "18. **EntropyRR, EntropyRG, EntropyRB**: Randomness or unpredictability of the color distribution in the date.\n",
        "19. **ALLdaub4RR, ALLdaub4RG, ALLdaub4RB**:\n",
        "20. **Class**: The type of the date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn2YKykPw0Xm"
      },
      "source": [
        "# 1B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9zzTm4QLWYC"
      },
      "source": [
        "check for duplicate rows, or incorrect data types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGxlmzU2ye51",
        "outputId": "8a8b44fa-e9c7-4ebb-d5a7-86eecdefada0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 898 entries, 0 to 897\n",
            "Data columns (total 35 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   AREA           898 non-null    int64  \n",
            " 1   PERIMETER      898 non-null    float64\n",
            " 2   MAJOR_AXIS     898 non-null    float64\n",
            " 3   MINOR_AXIS     898 non-null    float64\n",
            " 4   ECCENTRICITY   898 non-null    float64\n",
            " 5   EQDIASQ        898 non-null    float64\n",
            " 6   SOLIDITY       898 non-null    float64\n",
            " 7   CONVEX_AREA    898 non-null    int64  \n",
            " 8   EXTENT         898 non-null    float64\n",
            " 9   ASPECT_RATIO   898 non-null    float64\n",
            " 10  ROUNDNESS      898 non-null    float64\n",
            " 11  COMPACTNESS    898 non-null    float64\n",
            " 12  SHAPEFACTOR_1  898 non-null    float64\n",
            " 13  SHAPEFACTOR_2  898 non-null    float64\n",
            " 14  SHAPEFACTOR_3  898 non-null    float64\n",
            " 15  SHAPEFACTOR_4  898 non-null    float64\n",
            " 16  MeanRR         898 non-null    float64\n",
            " 17  MeanRG         898 non-null    float64\n",
            " 18  MeanRB         898 non-null    float64\n",
            " 19  StdDevRR       898 non-null    float64\n",
            " 20  StdDevRG       898 non-null    float64\n",
            " 21  StdDevRB       898 non-null    float64\n",
            " 22  SkewRR         898 non-null    float64\n",
            " 23  SkewRG         898 non-null    float64\n",
            " 24  SkewRB         898 non-null    float64\n",
            " 25  KurtosisRR     898 non-null    float64\n",
            " 26  KurtosisRG     898 non-null    float64\n",
            " 27  KurtosisRB     898 non-null    float64\n",
            " 28  EntropyRR      898 non-null    float64\n",
            " 29  EntropyRG      898 non-null    int64  \n",
            " 30  EntropyRB      898 non-null    int64  \n",
            " 31  ALLdaub4RR     898 non-null    float64\n",
            " 32  ALLdaub4RG     898 non-null    float64\n",
            " 33  ALLdaub4RB     898 non-null    float64\n",
            " 34  Class          898 non-null    object \n",
            "dtypes: float64(30), int64(4), object(1)\n",
            "memory usage: 245.7+ KB\n"
          ]
        }
      ],
      "source": [
        "cd.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "UBInO7HZye51",
        "outputId": "a4e96344-e0e3-40ec-aaec-86dcf506e987"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AREA</th>\n",
              "      <th>PERIMETER</th>\n",
              "      <th>MAJOR_AXIS</th>\n",
              "      <th>MINOR_AXIS</th>\n",
              "      <th>ECCENTRICITY</th>\n",
              "      <th>EQDIASQ</th>\n",
              "      <th>SOLIDITY</th>\n",
              "      <th>CONVEX_AREA</th>\n",
              "      <th>EXTENT</th>\n",
              "      <th>ASPECT_RATIO</th>\n",
              "      <th>ROUNDNESS</th>\n",
              "      <th>COMPACTNESS</th>\n",
              "      <th>SHAPEFACTOR_1</th>\n",
              "      <th>SHAPEFACTOR_2</th>\n",
              "      <th>SHAPEFACTOR_3</th>\n",
              "      <th>SHAPEFACTOR_4</th>\n",
              "      <th>MeanRR</th>\n",
              "      <th>MeanRG</th>\n",
              "      <th>MeanRB</th>\n",
              "      <th>StdDevRR</th>\n",
              "      <th>StdDevRG</th>\n",
              "      <th>StdDevRB</th>\n",
              "      <th>SkewRR</th>\n",
              "      <th>SkewRG</th>\n",
              "      <th>SkewRB</th>\n",
              "      <th>KurtosisRR</th>\n",
              "      <th>KurtosisRG</th>\n",
              "      <th>KurtosisRB</th>\n",
              "      <th>EntropyRR</th>\n",
              "      <th>EntropyRG</th>\n",
              "      <th>EntropyRB</th>\n",
              "      <th>ALLdaub4RR</th>\n",
              "      <th>ALLdaub4RG</th>\n",
              "      <th>ALLdaub4RB</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [AREA, PERIMETER, MAJOR_AXIS, MINOR_AXIS, ECCENTRICITY, EQDIASQ, SOLIDITY, CONVEX_AREA, EXTENT, ASPECT_RATIO, ROUNDNESS, COMPACTNESS, SHAPEFACTOR_1, SHAPEFACTOR_2, SHAPEFACTOR_3, SHAPEFACTOR_4, MeanRR, MeanRG, MeanRB, StdDevRR, StdDevRG, StdDevRB, SkewRR, SkewRG, SkewRB, KurtosisRR, KurtosisRG, KurtosisRB, EntropyRR, EntropyRG, EntropyRB, ALLdaub4RR, ALLdaub4RG, ALLdaub4RB, Class]\n",
              "Index: []"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cd[cd.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDn_Wlvjye51",
        "outputId": "a85de193-236b-478d-c673-fc9368c3d4cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 35)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cd[cd.duplicated()].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw4ZQbyULjUo"
      },
      "source": [
        "No duplicated values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUfErWsDye51",
        "outputId": "f63660b8-d724-4cd5-ac00-02d36d17c93a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AREA             0\n",
              "PERIMETER        0\n",
              "MAJOR_AXIS       0\n",
              "MINOR_AXIS       0\n",
              "ECCENTRICITY     0\n",
              "EQDIASQ          0\n",
              "SOLIDITY         0\n",
              "CONVEX_AREA      0\n",
              "EXTENT           0\n",
              "ASPECT_RATIO     0\n",
              "ROUNDNESS        0\n",
              "COMPACTNESS      0\n",
              "SHAPEFACTOR_1    0\n",
              "SHAPEFACTOR_2    0\n",
              "SHAPEFACTOR_3    0\n",
              "SHAPEFACTOR_4    0\n",
              "MeanRR           0\n",
              "MeanRG           0\n",
              "MeanRB           0\n",
              "StdDevRR         0\n",
              "StdDevRG         0\n",
              "StdDevRB         0\n",
              "SkewRR           0\n",
              "SkewRG           0\n",
              "SkewRB           0\n",
              "KurtosisRR       0\n",
              "KurtosisRG       0\n",
              "KurtosisRB       0\n",
              "EntropyRR        0\n",
              "EntropyRG        0\n",
              "EntropyRB        0\n",
              "ALLdaub4RR       0\n",
              "ALLdaub4RG       0\n",
              "ALLdaub4RB       0\n",
              "Class            0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cd.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A1uWJ4YxSIs"
      },
      "source": [
        "No missing values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0EDLmG2xR9o"
      },
      "source": [
        "# 1C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "HJQIzQxZye51",
        "outputId": "a20e38bd-db2b-4183-964c-241d5f052767"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Class')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAABR4ElEQVR4nO3dd1gU5/428HvpKk0QRYSA2EHssUexgz1irNiCJUdsYPfYTxJQYzQaW04UjFGxRI0xUaMIYkGNBbHFimIBNaggKEh53j98mZ/rLmVhV5Y59+e65rqcZ56Z/c5s4faZmV2FEEKAiIiISKYMSroAIiIiIl1i2CEiIiJZY9ghIiIiWWPYISIiIllj2CEiIiJZY9ghIiIiWWPYISIiIllj2CEiIiJZY9ghIiIiWWPYIdKSyMhIKBQK7Ny5s6RLKZTHjx+jb9++sLW1hUKhwPLly4u9zeHDh8PFxaXY25Gr3NdIZGRkSZdSLKGhoVAoFDh79uwHeTyFQoH58+d/kMcieWLYoVIl90PWzMwMDx8+VFnu6emJunXrlkBlpU9AQAAOHjyImTNnYtOmTfDy8sqzr0KhyHP64osvPmDVH9bq1asRGhpa0mXky8XFJc/nJr/nVB/ExMTA19cXTk5OMDU1hY2NDTp27IiQkBBkZ2eXdHkkI0YlXQBRUWRkZCA4OBgrV64s6VJKrSNHjqBXr16YMmVKofp36tQJQ4cOVWmvWbOmtkvTG6tXr0aFChUwfPhwrWyvTZs2eP36NUxMTLSyvVwNGjTA5MmTVdodHBy0+jja9OOPP+KLL75ApUqVMGTIENSoUQMvX75EeHg4/Pz8kJCQgFmzZpV0mSQTDDtUKjVo0AD//e9/MXPmTL3+QNeFtLQ0lCtXrtjbefLkCaytrQvdv2bNmvD19S324/4vMzAwgJmZmda3W6VKlVL13Jw6dQpffPEFWrRogT/++AMWFhbSskmTJuHs2bO4fPlyCVZIcsPTWFQqzZo1C9nZ2QgODs633927d6FQKNSeinj/OoD58+dDoVDgxo0b8PX1hZWVFezs7DBnzhwIIXD//n306tULlpaWsLe3x9KlS9U+ZnZ2NmbNmgV7e3uUK1cOPXv2xP3791X6nT59Gl5eXrCyskLZsmXRtm1bnDhxQqlPbk1Xr17FoEGDUL58ebRu3Trffb5z5w4+++wz2NjYoGzZsmjevDl+//13aXnuqUAhBFatWiWd8tCVnJwcLF++HO7u7jAzM0OlSpUwZswYPH/+XKmfi4sLunfvjsjISDRp0gRlypSBh4eHdH3Lrl274OHhATMzMzRu3BgXLlxQeay///4bffv2hY2NDczMzNCkSRPs3btXqU/u/p84cQKBgYGws7NDuXLl8Omnn+Lp06dK9Vy5cgVHjx6VjpGnpycAIDMzEwsWLECNGjVgZmYGW1tbtG7dGocOHcr3WKi7Zif31OvVq1fRrl07lC1bFlWqVMHixYs1OMoFi42NxfDhw+Hq6gozMzPY29vj888/R1JSkkrfhw8fws/PDw4ODjA1NUXVqlXxr3/9C2/evFHql5GRke8xzMuCBQugUCiwefNmpaCTq0mTJvmOpt27dw9jx45FrVq1UKZMGdja2uKzzz7D3bt3lfoV5nlKTEzEiBEj4OjoCFNTU1SuXBm9evVS2RaVbhzZoVKpatWqGDp0KP773/9ixowZWh3d6d+/P+rUqYPg4GD8/vvv+PLLL2FjY4N169ahffv2WLRoETZv3owpU6bg448/Rps2bZTW/+qrr6BQKDB9+nQ8efIEy5cvR8eOHRETE4MyZcoAeHsKydvbG40bN8a8efNgYGCAkJAQtG/fHseOHUPTpk2VtvnZZ5+hRo0a+PrrryGEyLP2x48fo2XLlnj16hUmTJgAW1tbbNy4ET179sTOnTvx6aefok2bNti0aROGDBmS56kpddLT0/HPP/+otFtaWuZ7WmbMmDEIDQ3FiBEjMGHCBMTFxeH777/HhQsXcOLECRgbG0t9b926hUGDBmHMmDHw9fXFN998gx49emDt2rWYNWsWxo4dCwAICgpCv379cP36dRgYvP0/25UrV9CqVStUqVIFM2bMQLly5bB9+3b07t0bv/zyCz799FOlusaPH4/y5ctj3rx5uHv3LpYvX45x48Zh27ZtAIDly5dj/PjxMDc3x7///W8AQKVKlQC8DaFBQUEYOXIkmjZtipSUFJw9exbnz59Hp06dCnU83/X8+XN4eXmhT58+6NevH3bu3Inp06fDw8MD3t7eBa6fmZmp9rkpV66c9Jo7dOgQ7ty5gxEjRsDe3h5XrlzBDz/8gCtXruDUqVNS4H306BGaNm2KFy9eYPTo0ahduzYePnyInTt34tWrV0rPdUHHUJ1Xr14hPDwcbdq0wUcffaTpoQIA/PXXXzh58iQGDBgAR0dH3L17F2vWrIGnpyeuXr2KsmXLAijc8+Tj44MrV65g/PjxcHFxwZMnT3Do0CHEx8fzYns5EUSlSEhIiAAg/vrrL3H79m1hZGQkJkyYIC1v27atcHd3l+bj4uIEABESEqKyLQBi3rx50vy8efMEADF69GipLSsrSzg6OgqFQiGCg4Ol9ufPn4syZcqIYcOGSW0RERECgKhSpYpISUmR2rdv3y4AiO+++04IIUROTo6oUaOG6NKli8jJyZH6vXr1SlStWlV06tRJpaaBAwcW6vhMmjRJABDHjh2T2l6+fCmqVq0qXFxcRHZ2ttL++/v7F2q7APKctm7dKvUbNmyYcHZ2luaPHTsmAIjNmzcrbe/AgQMq7c7OzgKAOHnypNR28OBBAUCUKVNG3Lt3T2pft26dACAiIiKktg4dOggPDw+Rnp4uteXk5IiWLVuKGjVqSG25r6GOHTsqHf+AgABhaGgoXrx4IbW5u7uLtm3bqhyP+vXri27duhVw1FTlvkberbtt27YCgPjpp5+ktoyMDGFvby98fHwK3GbucVM3BQUFSf1evXqlsu7WrVsFABEVFSW1DR06VBgYGIi//vpLpX/u8dLkGL7v4sWLAoCYOHFigfuW6/33qrp9iY6OVjmOBT1Pz58/FwDEkiVLCl0LlU48jUWllqurK4YMGYIffvgBCQkJWtvuyJEjpX8bGhqiSZMmEELAz89Pare2tkatWrVw584dlfWHDh2qNDTft29fVK5cGX/88QeAt3eg3Lx5E4MGDUJSUhL++ecf/PPPP0hLS0OHDh0QFRWFnJwcpW0W9o6nP/74A02bNlU61WVubo7Ro0fj7t27uHr1auEOghq9evXCoUOHVKZ27drluc6OHTtgZWWFTp06Sfv5zz//oHHjxjA3N0dERIRSfzc3N7Ro0UKab9asGQCgffv2SqMAue25x//Zs2c4cuQI+vXrh5cvX0qPk5SUhC5duuDmzZsqd++NHj1a6fTdJ598guzsbNy7d6/AY2FtbY0rV67g5s2bBfYtDHNzc6VrbkxMTNC0aVO1ry91mjVrpva5GThwoNQnd4QH+L9RuubNmwMAzp8/D+DtKcc9e/agR48eaNKkicrjvH+6syjHMCUlBQDUnr4qrHf3JTMzE0lJSahevTqsra2lfQEKfp7KlCkDExMTREZGqpxWJXnhaSwq1WbPno1NmzYhODgY3333nVa2+f7QupWVFczMzFChQgWVdnXXO9SoUUNpXqFQoHr16tI1ALkfvMOGDcuzhuTkZJQvX16ar1q1aqFqv3fvnhQE3lWnTh1peVFvzXd0dETHjh01WufmzZtITk5GxYoV1S5/8uSJ0ry6Yw8ATk5Oattz/0DdunULQgjMmTMHc+bMyfOxqlSpkudj5R7vwvzRW7hwIXr16oWaNWuibt268PLywpAhQ1CvXr0C11XH0dFRJUiUL18esbGxhVq/QoUKBT43z549w4IFCxAWFqZy3JOTkwEAT58+RUpKSqFfI0U5hpaWlgCAly9fFuox1Hn9+jWCgoIQEhKChw8fKp3azd0XoODnydTUFIsWLcLkyZNRqVIlNG/eHN27d8fQoUNhb29f5PpI/zDsUKnm6uoKX19f/PDDD5gxY4bK8rwuvM3vOzwMDQ0L1QYg3+tn8pI7arNkyRI0aNBAbR9zc3Ol+Xf/J1ua5OTkoGLFiti8ebPa5XZ2dkrzeR3ngo5/7jGdMmUKunTporZv9erVNdpmftq0aYPbt2/j119/xZ9//okff/wRy5Ytw9q1a5VGBgtLm6+vvPTr1w8nT57E1KlT0aBBA5ibmyMnJwdeXl4qI4mFVZS6q1evDiMjI1y6dKlIjwm8vVYoJCQEkyZNQosWLWBlZQWFQoEBAwYo7UthnqdJkyahR48e2LNnDw4ePIg5c+YgKCgIR44cQcOGDYtcI+kXhh0q9WbPno2ff/4ZixYtUlmW+z/NFy9eKLUX5lRFUb0/ZC6EwK1bt6T/TVarVg3A2//hajpSUhBnZ2dcv35dpf3vv/+Wln9I1apVw+HDh9GqVSudBjZXV1cAgLGxsVaPaX53qdnY2GDEiBEYMWIEUlNT0aZNG8yfP79IYUfXnj9/jvDwcCxYsABz586V2t9/rdrZ2cHS0lKnt32XLVsW7du3x5EjR3D//n2VUbvC2LlzJ4YNG6Z0R2R6errK+xwo3PNUrVo1TJ48GZMnT8bNmzfRoEEDLF26FD///HOR9pH0D6/ZoVKvWrVq8PX1xbp165CYmKi0zNLSEhUqVEBUVJRS++rVq3VWz08//aQ0RL9z504kJCRId9U0btwY1apVwzfffIPU1FSV9Qtz625eunbtijNnziA6OlpqS0tLww8//AAXFxe4ubkVedtF0a9fP2RnZ+M///mPyrKsrCy1f5yKomLFivD09MS6devUXr9V1GNarlw5tTW+f/rS3Nwc1atXR0ZGRpEeR9dyR2DeH3F5/ydCDAwM0Lt3b/z2229qfwpCWyNN8+bNgxACQ4YMUfseOHfuHDZu3Jjn+oaGhiq1rFy5UmXEtqDn6dWrV0hPT1fqU61aNVhYWOjtc0lFw5EdkoV///vf2LRpE65fvw53d3elZSNHjkRwcDBGjhyJJk2aICoqCjdu3NBZLTY2NmjdujVGjBiBx48fY/ny5ahevTpGjRoF4O0flB9//BHe3t5wd3fHiBEjUKVKFTx8+BARERGwtLTEb7/9VqTHnjFjBrZu3Qpvb29MmDABNjY22LhxI+Li4vDLL79It2kXxY0bN9T+T7dSpUp53m7dtm1bjBkzBkFBQYiJiUHnzp1hbGyMmzdvYseOHfjuu+/Qt2/fItf0rlWrVqF169bw8PDAqFGj4OrqisePHyM6OhoPHjzAxYsXNd5m48aNsWbNGnz55ZeoXr06KlasiPbt28PNzQ2enp5o3LgxbGxscPbsWezcuRPjxo3Tyr5o6uHDh2qfG3Nzc/Tu3RuWlpZo06YNFi9ejMzMTFSpUgV//vkn4uLiVNb5+uuv8eeff6Jt27YYPXo06tSpg4SEBOzYsQPHjx/X6Iso89KyZUusWrUKY8eORe3atZW+QTkyMhJ79+7Fl19+mef63bt3x6ZNm2BlZQU3NzdER0fj8OHDsLW1VepX0PN048YNdOjQAf369YObmxuMjIywe/duPH78GAMGDCj2fpL+YNghWahevTp8fX3V/m9w7ty5ePr0KXbu3Int27fD29sb+/fvz/Oi2eKaNWsWYmNjERQUhJcvX6JDhw5YvXq19N0fwNsvkouOjsZ//vMffP/990hNTYW9vT2aNWuGMWPGFPmxK1WqhJMnT2L69OlYuXIl0tPTUa9ePfz222/o1q1bsfYr9w6f97Vt2zbf75ZZu3YtGjdujHXr1mHWrFkwMjKCi4sLfH190apVq2LV9C43NzecPXsWCxYsQGhoKJKSklCxYkU0bNhQ6dSNJubOnYt79+5h8eLFePnyJdq2bYv27dtjwoQJ2Lt3L/78809kZGTA2dkZX375JaZOnaq1/dFETEwMhgwZotLu7OyM3r17AwC2bNmC8ePHY9WqVRBCoHPnzti/f7/Kd1RVqVIFp0+fxpw5c7B582akpKSgSpUq8Pb2VnoNF9eYMWPw8ccfY+nSpfjpp5/w9OlTmJubo1GjRggJCcn3G6G/++47GBoaYvPmzUhPT0erVq1w+PBhleu1CnqenJycMHDgQISHh2PTpk0wMjJC7dq1sX37dvj4+GhtX6nkKYQ2r4AjIiIi0jO8ZoeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSN37ODt7+r8+jRI1hYWOT79fBERESkP4QQePnyJRwcHPL90lSGHQCPHj0q0u+zEBERUcm7f/8+HB0d81zOsAPAwsICwNuDZWlpWcLVEBERUWGkpKTAyclJ+jueF4Yd/N8vG1taWjLsEBERlTIFXYLCC5SJiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWjEq6gNKg8dSfSroEnTq3ZGhJl0BERKQzHNkhIiIiWSvRsBMUFISPP/4YFhYWqFixInr37o3r168r9UlPT4e/vz9sbW1hbm4OHx8fPH78WKlPfHw8unXrhrJly6JixYqYOnUqsrKyPuSuEBERkZ4q0bBz9OhR+Pv749SpUzh06BAyMzPRuXNnpKWlSX0CAgLw22+/YceOHTh69CgePXqEPn36SMuzs7PRrVs3vHnzBidPnsTGjRsRGhqKuXPnlsQuERERkZ5RCCFESReR6+nTp6hYsSKOHj2KNm3aIDk5GXZ2dtiyZQv69u0LAPj7779Rp04dREdHo3nz5ti/fz+6d++OR48eoVKlSgCAtWvXYvr06Xj69ClMTEwKfNyUlBRYWVkhOTkZlpaWKst5zQ4REZH+Kejvdy69umYnOTkZAGBjYwMAOHfuHDIzM9GxY0epT+3atfHRRx8hOjoaABAdHQ0PDw8p6ABAly5dkJKSgitXrnzA6omIiEgf6c3dWDk5OZg0aRJatWqFunXrAgASExNhYmICa2trpb6VKlVCYmKi1OfdoJO7PHeZOhkZGcjIyJDmU1JStLUbREREpGf0ZmTH398fly9fRlhYmM4fKygoCFZWVtLk5OSk88ckIiKikqEXIzvjxo3Dvn37EBUVBUdHR6nd3t4eb968wYsXL5RGdx4/fgx7e3upz5kzZ5S2l3u3Vm6f982cOROBgYHSfEpKCgNPEcUv9CjpEnTqo7mXSroEIiIqphId2RFCYNy4cdi9ezeOHDmCqlWrKi1v3LgxjI2NER4eLrVdv34d8fHxaNGiBQCgRYsWuHTpEp48eSL1OXToECwtLeHm5qb2cU1NTWFpaak0ERERkTyV6MiOv78/tmzZgl9//RUWFhbSNTZWVlYoU6YMrKys4Ofnh8DAQNjY2MDS0hLjx49HixYt0Lx5cwBA586d4ebmhiFDhmDx4sVITEzE7Nmz4e/vD1NT05LcPSIiItIDJRp21qxZAwDw9PRUag8JCcHw4cMBAMuWLYOBgQF8fHyQkZGBLl26YPXq1VJfQ0ND7Nu3D//617/QokULlCtXDsOGDcPChQs/1G4QERGRHivRsFOYr/gxMzPDqlWrsGrVqjz7ODs7448//tBmaURERCQTenM3FhEREZEuMOwQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrJVo2ImKikKPHj3g4OAAhUKBPXv2KC1XKBRqpyVLlkh9XFxcVJYHBwd/4D0hIiIifVWiYSctLQ3169fHqlWr1C5PSEhQmjZs2ACFQgEfHx+lfgsXLlTqN378+A9RPhEREZUCRiX54N7e3vD29s5zub29vdL8r7/+inbt2sHV1VWp3cLCQqUvEREREVCKrtl5/Pgxfv/9d/j5+aksCw4Ohq2tLRo2bIglS5YgKyurBCokIiIifVSiIzua2LhxIywsLNCnTx+l9gkTJqBRo0awsbHByZMnMXPmTCQkJODbb7/Nc1sZGRnIyMiQ5lNSUnRWNxEREZWsUhN2NmzYgMGDB8PMzEypPTAwUPp3vXr1YGJigjFjxiAoKAimpqZqtxUUFIQFCxbotF4iIiLSD6XiNNaxY8dw/fp1jBw5ssC+zZo1Q1ZWFu7evZtnn5kzZyI5OVma7t+/r8VqiYiISJ+UipGd9evXo3Hjxqhfv36BfWNiYmBgYICKFSvm2cfU1DTPUR8iIiKSlxINO6mpqbh165Y0HxcXh5iYGNjY2OCjjz4C8PZ6mh07dmDp0qUq60dHR+P06dNo164dLCwsEB0djYCAAPj6+qJ8+fIfbD+IiIhIf5Vo2Dl79izatWsnzedefzNs2DCEhoYCAMLCwiCEwMCBA1XWNzU1RVhYGObPn4+MjAxUrVoVAQEBStfxEBER0f+2Eg07np6eEELk22f06NEYPXq02mWNGjXCqVOndFEaERERyUSpuECZiIiIqKgYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1krFD4ESlTatVrYq6RJ07sT4EyVdAhFRoXBkh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkrUTDTlRUFHr06AEHBwcoFArs2bNHafnw4cOhUCiUJi8vL6U+z549w+DBg2FpaQlra2v4+fkhNTX1A+4FERER6bMSDTtpaWmoX78+Vq1alWcfLy8vJCQkSNPWrVuVlg8ePBhXrlzBoUOHsG/fPkRFRWH06NG6Lp2IiIhKCaOSfHBvb294e3vn28fU1BT29vZql127dg0HDhzAX3/9hSZNmgAAVq5cia5du+Kbb76Bg4OD1msmIiKi0kXvr9mJjIxExYoVUatWLfzrX/9CUlKStCw6OhrW1tZS0AGAjh07wsDAAKdPn85zmxkZGUhJSVGaiIiISJ70Oux4eXnhp59+Qnh4OBYtWoSjR4/C29sb2dnZAIDExERUrFhRaR0jIyPY2NggMTExz+0GBQXByspKmpycnHS6H0RERFRySvQ0VkEGDBgg/dvDwwP16tVDtWrVEBkZiQ4dOhR5uzNnzkRgYKA0n5KSwsBDREQkU3o9svM+V1dXVKhQAbdu3QIA2Nvb48mTJ0p9srKy8OzZszyv8wHeXgdkaWmpNBEREZE8laqw8+DBAyQlJaFy5coAgBYtWuDFixc4d+6c1OfIkSPIyclBs2bNSqpMIiIi0iMah50DBw7g+PHj0vyqVavQoEEDDBo0CM+fP9doW6mpqYiJiUFMTAwAIC4uDjExMYiPj0dqaiqmTp2KU6dO4e7duwgPD0evXr1QvXp1dOnSBQBQp04deHl5YdSoUThz5gxOnDiBcePGYcCAAbwTi4iIiAAUIexMnTpVunvp0qVLmDx5Mrp27Yq4uDil62AK4+zZs2jYsCEaNmwIAAgMDETDhg0xd+5cGBoaIjY2Fj179kTNmjXh5+eHxo0b49ixYzA1NZW2sXnzZtSuXRsdOnRA165d0bp1a/zwww+a7hYRERHJlMYXKMfFxcHNzQ0A8Msvv6B79+74+uuvcf78eXTt2lWjbXl6ekIIkefygwcPFrgNGxsbbNmyRaPHJSIiov8dGo/smJiY4NWrVwCAw4cPo3PnzgDehg5+Xw0RERHpG41Hdlq3bo3AwEC0atUKZ86cwbZt2wAAN27cgKOjo9YLJCIiIioOjUd2vv/+exgZGWHnzp1Ys2YNqlSpAgDYv3+/yo90EhEREZU0jUd2PvroI+zbt0+lfdmyZVopiIiIiEibNB7ZMTQ0VPkiPwBISkqCoaGhVooiIiIi0haNw05ed09lZGTAxMSk2AURERERaVOhT2OtWLECAKBQKPDjjz/C3NxcWpadnY2oqCjUrl1b+xUSERERFUOhw07uNTlCCKxdu1bplJWJiQlcXFywdu1a7VdIREREVAyFDjtxcXEAgHbt2mHXrl0oX768zooiIiIi0haN78aKiIjQRR1EREREOqFx2MnOzkZoaCjCw8Px5MkT5OTkKC0/cuSI1oojIiIiKi6Nw87EiRMRGhqKbt26oW7dulAoFLqoi4iIiEgrNA47YWFh2L59u8Y/+klERERUEor0Q6DVq1fXRS1EREREWqdx2Jk8eTK+++67PL9ckIiIiEifaHwa6/jx44iIiMD+/fvh7u4OY2NjpeW7du3SWnFERERExaVx2LG2tsann36qi1qIiIiItE7jsBMSEqKLOoiIiIh0QuNrdgAgKysLhw8fxrp16/Dy5UsAwKNHj5CamqrV4oiIiIiKS+ORnXv37sHLywvx8fHIyMhAp06dYGFhgUWLFiEjI4O/j0VERER6ReORnYkTJ6JJkyZ4/vw5ypQpI7V/+umnCA8P12pxRERERMWl8cjOsWPHcPLkSZiYmCi1u7i44OHDh1orjIiIiEgbNB7ZycnJQXZ2tkr7gwcPYGFhoZWiiIiIiLRF47DTuXNnLF++XJpXKBRITU3FvHnz+BMSREREpHc0Po21dOlSdOnSBW5ubkhPT8egQYNw8+ZNVKhQAVu3btVFjURERERFpnHYcXR0xMWLFxEWFobY2FikpqbCz88PgwcPVrpgmYiIiEgfaBx2AMDIyAi+vr7aroWIiIhI64oUdh49eoTjx4/jyZMnyMnJUVo2YcIErRRGREREpA0ah53Q0FCMGTMGJiYmsLW1hUKhkJYpFAqGHSIiItIrGoedOXPmYO7cuZg5cyYMDIr0axNEREREH4zGaeXVq1cYMGAAgw4RERGVChonFj8/P+zYsUMXtRARERFpncZhJygoCEePHoWnpyfGjx+PwMBApUkTUVFR6NGjBxwcHKBQKLBnzx5pWWZmJqZPnw4PDw+UK1cODg4OGDp0KB49eqS0DRcXFygUCqUpODhY090iIiIimdL4mp2goCAcPHgQtWrVAgCVC5Q1kZaWhvr16+Pzzz9Hnz59lJa9evUK58+fx5w5c1C/fn08f/4cEydORM+ePXH27FmlvgsXLsSoUaOkef5sBREREeUq0jcob9iwAcOHDy/2g3t7e8Pb21vtMisrKxw6dEip7fvvv0fTpk0RHx+Pjz76SGq3sLCAvb19seshIiIi+dH4NJapqSlatWqli1oKlJycDIVCAWtra6X24OBg2NraomHDhliyZAmysrLy3U5GRgZSUlKUJiIiIpInjcPOxIkTsXLlSl3Ukq/09HRMnz4dAwcOhKWlpdQ+YcIEhIWFISIiAmPGjMHXX3+NadOm5butoKAgWFlZSZOTk5OuyyciIqISovFprDNnzuDIkSPYt28f3N3dYWxsrLR8165dWisuV2ZmJvr16wchBNasWaO07N2LouvVqwcTExOMGTMGQUFBMDU1Vbu9mTNnKq2XkpLCwENERCRTGocda2trlYuJdSk36Ny7dw9HjhxRGtVRp1mzZsjKysLdu3eli6jfZ2pqmmcQIiIiInnROOyEhIToog61coPOzZs3ERERAVtb2wLXiYmJgYGBASpWrPgBKiQiIiJ9V6QfAs3KykJkZCRu376NQYMGwcLCAo8ePYKlpSXMzc0LvZ3U1FTcunVLmo+Li0NMTAxsbGxQuXJl9O3bF+fPn8e+ffuQnZ2NxMREAICNjQ1MTEwQHR2N06dPo127drCwsEB0dDQCAgLg6+uL8uXLF2XXiIiISGY0Djv37t2Dl5cX4uPjkZGRgU6dOsHCwgKLFi1CRkYG1q5dW+htnT17Fu3atZPmc6+jGTZsGObPn4+9e/cCABo0aKC0XkREBDw9PWFqaoqwsDDMnz8fGRkZqFq1KgICAjT+ckMiIiKSL43DzsSJE9GkSRNcvHhR6bTSp59+qvTFfoXh6ekJIUSey/NbBgCNGjXCqVOnNHpMIiIi+t+icdg5duwYTp48CRMTE6V2FxcXPHz4UGuFEREREWmDxt+zk5OTg+zsbJX2Bw8e8GcaiIiISO9oHHY6d+6M5cuXS/MKhQKpqamYN28eunbtqs3aiIiIiIqtSL+N1aVLF7i5uSE9PR2DBg3CzZs3UaFCBWzdulUXNRIREREVmcZhx9HRERcvXkRYWBhiY2ORmpoKPz8/DB48GGXKlNFFjURERERFVqTv2TEyMoKvr6+2ayEiIiLSukKFndzvuymMnj17FrkYIiIiIm0rVNjp3bt3oTamUCjU3qlFREREVFIKFXZycnJ0XQcRERGRTmh86zkRERFRaVLosNO1a1ckJydL88HBwXjx4oU0n5SUBDc3N60WR0RERFRchQ47Bw8eREZGhjT/9ddf49mzZ9J8VlYWrl+/rt3qiIiIiIqp0GHn/R/lLOhHOomIiIj0Aa/ZISIiIlkrdNhRKBRQKBQqbURERET6rNDfoCyEwPDhw2FqagoASE9PxxdffIFy5coBgNL1PERERET6otBhZ9iwYUrz6n4uYujQocWviIiIiEiLCh12QkJCdFkHERERkU7wAmUiIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKStUKFnUaNGuH58+cAgIULF+LVq1c6LYqIiIhIWwoVdq5du4a0tDQAwIIFC5CamqrTooiIiIi0pVC3njdo0AAjRoxA69atIYTAN998A3Nzc7V9586dq9UCiYiIiIqjUGEnNDQU8+bNw759+6BQKLB//34YGamuqlAoGHaIiIhIrxQq7NSqVQthYWEAAAMDA4SHh6NixYo6LYyIiIhIGwr9Dcq5cnJydFEHERERkU5oHHYA4Pbt21i+fDmuXbsGAHBzc8PEiRNRrVo1rRZHREREVFwaf8/OwYMH4ebmhjNnzqBevXqoV68eTp8+DXd3dxw6dEgXNRIREREVmcYjOzNmzEBAQACCg4NV2qdPn45OnTpprTgiIiKi4tJ4ZOfatWvw8/NTaf/8889x9epVrRRFREREpC0ahx07OzvExMSotMfExGh8h1ZUVBR69OgBBwcHKBQK7NmzR2m5EAJz585F5cqVUaZMGXTs2BE3b95U6vPs2TMMHjwYlpaWsLa2hp+fH7/0kIiIiCQah51Ro0Zh9OjRWLRoEY4dO4Zjx44hODgYY8aMwahRozTaVlpaGurXr49Vq1apXb548WKsWLECa9euxenTp1GuXDl06dIF6enpUp/BgwfjypUrOHToEPbt24eoqCiMHj1a090iIiIimdL4mp05c+bAwsICS5cuxcyZMwEADg4OmD9/PiZMmKDRtry9veHt7a12mRACy5cvx+zZs9GrVy8AwE8//YRKlSphz549GDBgAK5du4YDBw7gr7/+QpMmTQAAK1euRNeuXfHNN9/AwcFB090jIiIimdF4ZEehUCAgIAAPHjxAcnIykpOT8eDBA0ycOBEKhUJrhcXFxSExMREdO3aU2qysrNCsWTNER0cDAKKjo2FtbS0FHQDo2LEjDAwMcPr06Ty3nZGRgZSUFKWJiIiI5EnjsPMuCwsLWFhYaKsWJYmJiQCASpUqKbVXqlRJWpaYmKhynZCRkRFsbGykPuoEBQXByspKmpycnLRcPREREemLYoWd0mrmzJnSqFRycjLu379f0iURERGRjuht2LG3twcAPH78WKn98ePH0jJ7e3s8efJEaXlWVhaePXsm9VHH1NQUlpaWShMRERHJk96GnapVq8Le3h7h4eFSW0pKCk6fPo0WLVoAAFq0aIEXL17g3LlzUp8jR44gJycHzZo1++A1ExERkf7R6G6szMxMeHl5Ye3atahRo0axHzw1NRW3bt2S5uPi4hATEwMbGxt89NFHmDRpEr788kvUqFEDVatWxZw5c+Dg4IDevXsDAOrUqQMvLy+MGjUKa9euRWZmJsaNG4cBAwbwTiwiPXW0TduSLkHn2kYdLekSiOgdGoUdY2NjxMbGau3Bz549i3bt2knzgYGBAIBhw4YhNDQU06ZNQ1paGkaPHo0XL16gdevWOHDgAMzMzKR1Nm/ejHHjxqFDhw4wMDCAj48PVqxYobUaiYiIqHTT+Ht2fH19sX79epXfxioKT09PCCHyXK5QKLBw4UIsXLgwzz42NjbYsmVLsWshIiIiedI47GRlZWHDhg04fPgwGjdujHLlyikt//bbb7VWHBEREVFxaRx2Ll++jEaNGgEAbty4obRMm18qSERERKQNGoediIgIXdRBREREpBNFvvX81q1bOHjwIF6/fg0A+V57Q0RERFRSNB7ZSUpKQr9+/RAREQGFQoGbN2/C1dUVfn5+KF++PJYuXaqLOomIZO/7yb+VdAk6N25pj5Iugf4HaTyyExAQAGNjY8THx6Ns2bJSe//+/XHgwAGtFkdERERUXBqP7Pz55584ePAgHB0dldpr1KiBe/fuaa0wIiIiIm3QeGQnLS1NaUQn17Nnz2BqaqqVooiIiIi0ReOw88knn+Cnn36S5hUKBXJycrB48WKlb0MmIiIi0gcan8ZavHgxOnTogLNnz+LNmzeYNm0arly5gmfPnuHEiRO6qJGIiIioyDQe2albty5u3LiB1q1bo1evXkhLS0OfPn1w4cIFVKtWTRc1EhERERWZxiM7AGBlZYV///vf2q6FiIiISOuKFHaeP3+O9evX49q1awAANzc3jBgxAjY2NlotjoiIiKi4ND6NFRUVBRcXF6xYsQLPnz/H8+fPsWLFClStWhVRUVG6qJGIiIioyDQe2fH390f//v2xZs0aGBoaAgCys7MxduxY+Pv749KlS1ovkoiIiKioNB7ZuXXrFiZPniwFHQAwNDREYGAgbt26pdXiiIiIiIpL47DTqFEj6Vqdd127dg3169fXSlFERERE2lKo01ixsbHSvydMmICJEyfi1q1baN68OQDg1KlTWLVqFYKDg3VTJREREVERFSrsNGjQAAqFAkIIqW3atGkq/QYNGoT+/ftrrzoiIiKiYipU2ImLi9N1HUREREQ6Uaiw4+zsrOs6iIiIiHSiSF8q+OjRIxw/fhxPnjxBTk6O0rIJEyZopTAiIiIibdA47ISGhmLMmDEwMTGBra0tFAqFtEyhUDDsEBERkV7ROOzMmTMHc+fOxcyZM2FgoPGd60REREQflMZp5dWrVxgwYACDDhEREZUKGicWPz8/7NixQxe1EBEREWmdxqexgoKC0L17dxw4cAAeHh4wNjZWWv7tt99qrTgiIiKi4ipS2Dl48CBq1aoFACoXKBMRERHpE43DztKlS7FhwwYMHz5cB+UQERERaZfG1+yYmpqiVatWuqiFiIiISOs0DjsTJ07EypUrdVELERERkdZpfBrrzJkzOHLkCPbt2wd3d3eVC5R37dqlteKIiIiIikvjkR1ra2v06dMHbdu2RYUKFWBlZaU0aZuLiwsUCoXK5O/vDwDw9PRUWfbFF19ovQ4iIiIqnTQe2QkJCdFFHXn666+/kJ2dLc1fvnwZnTp1wmeffSa1jRo1CgsXLpTmy5Yt+0FrJCIiIv1VpB8C/ZDs7OyU5oODg1GtWjW0bdtWaitbtizs7e0/dGlERERUCmgcdqpWrZrv9+ncuXOnWAXl582bN/j5558RGBioVMPmzZvx888/w97eHj169MCcOXPyHd3JyMhARkaGNJ+SkqKzmomIiKhkaRx2Jk2apDSfmZmJCxcu4MCBA5g6daq26lJrz549ePHihdJ3/AwaNAjOzs5wcHBAbGwspk+fjuvXr+d7oXRQUBAWLFig01qJiIhIP2gcdiZOnKi2fdWqVTh79myxC8rP+vXr4e3tDQcHB6lt9OjR0r89PDxQuXJldOjQAbdv30a1atXUbmfmzJkIDAyU5lNSUuDk5KS7womIiKjEaO2ny729vfHLL79oa3Mq7t27h8OHD2PkyJH59mvWrBkA4NatW3n2MTU1haWlpdJERERE8qS1sLNz507Y2Nhoa3MqQkJCULFiRXTr1i3ffjExMQCAypUr66wWIiIiKj00Po3VsGFDpYuDhRBITEzE06dPsXr1aq0WlysnJwchISEYNmwYjIz+r+Tbt29jy5Yt6Nq1K2xtbREbG4uAgAC0adMG9erV00ktREREVLpoHHZ69+6tNG9gYAA7Ozt4enqidu3a2qpLyeHDhxEfH4/PP/9cqd3ExASHDx/G8uXLkZaWBicnJ/j4+GD27Nk6qYOIiErGV759S7oEnfv3zztLugTZ0jjszJs3Txd15Ktz584QQqi0Ozk54ejRox+8HiIiIio9tHbNDhEREZE+KvTIjoGBQb5fJggACoUCWVlZxS6KiIiISFsKHXZ2796d57Lo6GisWLECOTk5WimKiIiISFsKHXZ69eql0nb9+nXMmDEDv/32GwYPHqz0Y5xERERE+qBI1+w8evQIo0aNgoeHB7KyshATE4ONGzfC2dlZ2/URERERFYtGYSc5ORnTp09H9erVceXKFYSHh+O3335D3bp1dVUfERERUbEU+jTW4sWLsWjRItjb22Pr1q1qT2sRERER6ZtCh50ZM2agTJkyqF69OjZu3IiNGzeq7Zffr40TERERfWiFDjtDhw4t8NZzIiIiIn1T6LATGhqqwzKIiIiIdIPfoExERESypvFvYxEREZH+uPbVkZIuQefq/Lt9sdbnyA4RERHJGsMOERERyRrDDhEREckaww4RERHJGsMOERERyRrDDhEREckaww4RERHJGsMOERERyRrDDhEREckaww4RERHJGsMOERERyRrDDhEREckaww4RERHJGsMOERERyRrDDhEREckaww4RERHJGsMOERERyRrDDhEREckaww4RERHJGsMOERERyZpeh5358+dDoVAoTbVr15aWp6enw9/fH7a2tjA3N4ePjw8eP35cghUTERGRvtHrsAMA7u7uSEhIkKbjx49LywICAvDbb79hx44dOHr0KB49eoQ+ffqUYLVERESkb4xKuoCCGBkZwd7eXqU9OTkZ69evx5YtW9C+fXsAQEhICOrUqYNTp06hefPmH7pUIiIi0kN6P7Jz8+ZNODg4wNXVFYMHD0Z8fDwA4Ny5c8jMzETHjh2lvrVr18ZHH32E6OjofLeZkZGBlJQUpYmIiIjkSa/DTrNmzRAaGooDBw5gzZo1iIuLwyeffIKXL18iMTERJiYmsLa2VlqnUqVKSExMzHe7QUFBsLKykiYnJycd7gURERGVJL0+jeXt7S39u169emjWrBmcnZ2xfft2lClTpsjbnTlzJgIDA6X5lJQUBh4iIiKZ0uuRnfdZW1ujZs2auHXrFuzt7fHmzRu8ePFCqc/jx4/VXuPzLlNTU1haWipNREREJE+lKuykpqbi9u3bqFy5Mho3bgxjY2OEh4dLy69fv474+Hi0aNGiBKskIiIifaLXp7GmTJmCHj16wNnZGY8ePcK8efNgaGiIgQMHwsrKCn5+fggMDISNjQ0sLS0xfvx4tGjRgndiERERkUSvw86DBw8wcOBAJCUlwc7ODq1bt8apU6dgZ2cHAFi2bBkMDAzg4+ODjIwMdOnSBatXry7hqomIiEif6HXYCQsLy3e5mZkZVq1ahVWrVn2gioiIiKi0KVXX7BARERFpimGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZE2vw05QUBA+/vhjWFhYoGLFiujduzeuX7+u1MfT0xMKhUJp+uKLL0qoYiIiItI3eh12jh49Cn9/f5w6dQqHDh1CZmYmOnfujLS0NKV+o0aNQkJCgjQtXry4hComIiIifWNU0gXk58CBA0rzoaGhqFixIs6dO4c2bdpI7WXLloW9vf2HLo+IiIhKAb0e2XlfcnIyAMDGxkapffPmzahQoQLq1q2LmTNn4tWrVyVRHhEREekhvR7ZeVdOTg4mTZqEVq1aoW7dulL7oEGD4OzsDAcHB8TGxmL69Om4fv06du3alee2MjIykJGRIc2npKTotHYiIiIqOaUm7Pj7++Py5cs4fvy4Uvvo0aOlf3t4eKBy5cro0KEDbt++jWrVqqndVlBQEBYsWKDTeomIiEg/lIrTWOPGjcO+ffsQEREBR0fHfPs2a9YMAHDr1q08+8ycORPJycnSdP/+fa3WS0RERPpDr0d2hBAYP348du/ejcjISFStWrXAdWJiYgAAlStXzrOPqakpTE1NtVUmERER6TG9Djv+/v7YsmULfv31V1hYWCAxMREAYGVlhTJlyuD27dvYsmULunbtCltbW8TGxiIgIABt2rRBvXr1Srh6IiIi0gd6HXbWrFkD4O0XB74rJCQEw4cPh4mJCQ4fPozly5cjLS0NTk5O8PHxwezZs0ugWiIiItJHeh12hBD5LndycsLRo0c/UDVERERUGpWKC5SJiIiIiophh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGRNNmFn1apVcHFxgZmZGZo1a4YzZ86UdElERESkB2QRdrZt24bAwEDMmzcP58+fR/369dGlSxc8efKkpEsjIiKiEiaLsPPtt99i1KhRGDFiBNzc3LB27VqULVsWGzZsKOnSiIiIqISV+rDz5s0bnDt3Dh07dpTaDAwM0LFjR0RHR5dgZURERKQPjEq6gOL6559/kJ2djUqVKim1V6pUCX///bfadTIyMpCRkSHNJycnAwBSUlLU9s/OeK2lavVTXvtdGC/Ts7VYif4p6rHJep2l5Ur0T1GPTVoWj01eXme80nIl+qeoxyY9M1PLleifoh6b1PQ0LVeif/I6NrntQoj8NyBKuYcPHwoA4uTJk0rtU6dOFU2bNlW7zrx58wQATpw4ceLEiZMMpvv37+ebFUr9yE6FChVgaGiIx48fK7U/fvwY9vb2ateZOXMmAgMDpfmcnBw8e/YMtra2UCgUOq23ICkpKXBycsL9+/dhaWlZorXoGx6bvPHY5I3HJm88NurxuORN346NEAIvX76Eg4NDvv1KfdgxMTFB48aNER4ejt69ewN4G17Cw8Mxbtw4teuYmprC1NRUqc3a2lrHlWrG0tJSL15I+ojHJm88Nnnjsckbj416PC5506djY2VlVWCfUh92ACAwMBDDhg1DkyZN0LRpUyxfvhxpaWkYMWJESZdGREREJUwWYad///54+vQp5s6di8TERDRo0AAHDhxQuWiZiIiI/vfIIuwAwLhx4/I8bVWamJqaYt68eSqn2YjHJj88Nnnjsckbj416PC55K63HRiFEQfdrEREREZVepf5LBYmIiIjyw7BDREREssawQ0RERLLGsENERESyxrBTRMOHD4dCoYBCoYCxsTEqVaqETp06YcOGDcjJyVHqe/LkSXTt2hXly5eHmZkZPDw88O233yI7W/l3pRQKBfbs2SPNZ2ZmYuDAgahSpQouX74MALhy5Qr69esHOzs7mJqaombNmpg7dy5evVL+TR0XFxcsX75cJ/teGE+fPsW//vUvfPTRRzA1NYW9vT26dOmCEydOKPWLjo6GoaEhunXrprKNu3fvSsf43cnX11fq8/r1a9jY2KBChQpKv3fm4eGBL774Qm1tmzZtgqmpKf755x9ERkZCoVDgxYsX2tnx97z/OqlatSqmTZuG9PR0pX779u1D27ZtYWFhgbJly+Ljjz9GaGiotHz+/Plqj8W7U66tW7fC0NAQ/v7+KvXk7q+7u7vK68/a2lrpMXMFBQXB0NAQS5YsKd7BeE9h30MuLi5q9zc4OFhpe7/88gvat2+P8uXLo0yZMqhVqxY+//xzXLhwQeoTGhqa7xeIvlvTu5OXl5d07PKbIiMjtXqM3lWY91RhP2sAICIiAt27d4ednR3MzMxQrVo19O/fH1FRUWofv3bt2jA1NUViYiKAt79LaG9vj6+//lqlb79+/dC8eXO1j/shDB8+XPqS2cK+BwHgwYMHMDExQd26ddVuNzs7G8uWLYOHhwfMzMxQvnx5eHt7q3yuFfQ6+9Def13b2trCy8sLsbGxUp+8XtNhYWEAoPL6t7OzQ9euXXHp0iWVx8o99u96/7NW15+972PYKQYvLy8kJCTg7t272L9/P9q1a4eJEyeie/fuyPr/P3a4e/dutG3bFo6OjoiIiMDff/+NiRMn4ssvv8SAAQPy/PGyV69eoWfPnvjrr79w/Phx1K1bF6dOnUKzZs3w5s0b/P7777hx4wa++uorhIaGolOnTnjz5s2H3P18+fj44MKFC9i4cSNu3LiBvXv3wtPTE0lJSUr91q9fj/HjxyMqKgqPHj1Su63Dhw8jISFBmlatWiUt++WXX+Du7o7atWsrBUU/Pz+EhYXh9WvVH3ENCQlBz549UaFCBe3sbAFyXyd37tzBsmXLsG7dOsybN09avnLlSvTq1QutWrXC6dOnERsbiwEDBuCLL77AlClTAABTpkxROgaOjo5YuHChUluu9evXY9q0adi6davaD3QAuHPnDn766adC1b9hwwZMmzYNGzZsKMZRUK8w7yEAKvuakJCA8ePHS8unT5+O/v37o0GDBti7dy+uX7+OLVu2wNXVFTNnzixSTe9OW7duRcuWLZXa+vXrp9K3ZcuWWjs27yvoPaXJZ83q1avRoUMH2NraYtu2bbh+/Tp2796Nli1bIiAgQOWxjx8/jtevX6Nv377YuHEjgLc/1fPDDz9gwYIFSn/wduzYgX379mHjxo0wNDTU2fHQREHvwVyhoaHo168fUlJScPr0aaVlQggMGDAACxcuxMSJE3Ht2jVERkbCyckJnp6eSp8/+ujd12p4eDiMjIzQvXt3pT4hISEqr/33g8v169eRkJCAgwcPIiMjA926ddOrvz150sJvcf5PGjZsmOjVq5dKe3h4uAAg/vvf/4rU1FRha2sr+vTpo9Jv7969AoAICwuT2gCI3bt3i+fPn4uWLVuKevXqiYSEBCGEEDk5OcLNzU00adJEZGdnK20rJiZGKBQKERwcLLU5OzuLZcuWaWdnNfT8+XMBQERGRubb7+XLl8Lc3Fz8/fffon///uKrr75SWh4XFycAiAsXLuS5DU9PT7F27VqxZs0a0alTJ6n96dOnwsTERGzatEmp/507d4RCoRD79+8XQggREREhAIjnz59rtpOFpO510qdPH9GwYUMhhBDx8fHC2NhYBAYGqqy7YsUKAUCcOnVKZVlez++dO3dEmTJlxIsXL0SzZs3E5s2blZbn7u/UqVOFk5OTSE9Pl5ZZWVmJkJAQpf6RkZGiSpUq4s2bN8LBwUGcOHGikHtesMK8h4Qo+LUcHR0tAIjvvvtO7fKcnBzp3yEhIcLKykrjmorbt7gKek9p8llz7949YWxsLAICAtRu693jlWv48OFixowZYv/+/aJmzZoqyxo2bCjevHkjnjx5Iuzs7PJ8Lj6Ud5+bgt6DuXJycoSrq6s4cOCAmD59uhg1apTS8rCwMAFA7N27V+Xx+vTpI2xtbUVqaqoQouDX2Yem7hgcO3ZMABBPnjwRQvzf35+8qPuszH1tXbx4Md/HUre+rj9738eRHS1r37496tevj127duHPP/9EUlKS9L/zd/Xo0QM1a9bE1q1bldoTExPRtm1bAMDRo0elHzONiYnB1atXERgYCAMD5aetfv366Nixo8q2Soq5uTnMzc2xZ88epVNL79u+fTtq166NWrVqwdfXFxs2bMhzpEud27dvIzo6Gv369UO/fv1w7Ngx3Lt3D8Db/3X26tVLZTQiNDQUjo6O6Ny5c9F2rpguX76MkydPwsTEBACwc+dOZGZmqn2NjBkzBubm5ho9ryEhIejWrRusrKzg6+uL9evXq+03adIkZGVlYeXKlflub/369Rg4cCCMjY0xcODAPLenTe++hwpj69atMDc3x9ixY9UuL+kf99WGgt5TmnzW/PLLL8jMzMS0adPUPtb7x+vly5fYsWMHfH190alTJyQnJ+PYsWPS8u+++w5JSUn4z3/+g7Fjx6Ju3bpKo2765v33YK6IiAi8evUKHTt2hK+vL8LCwpCWliYt37JlC2rWrIkePXqobHPy5MlISkrCoUOHdF6/NqSmpuLnn39G9erVYWtrW6RtJCcnS6e43j+W+ohhRwdq166Nu3fv4saNGwCAOnXq5Nkvt0+uiRMn4s2bNzh06JDSOd+CtlWnTh2VbZUUIyMjhIaGYuPGjbC2tkarVq0wa9YspfPDwNs/pLnX33h5eSE5ORlHjx5V2V7Lli2lD3tzc3PpGowNGzbA29sb5cuXh42NDbp06YKQkBBpPT8/P0RGRiIuLg7A22HojRs3YtiwYSqBUZf27dsHc3Nz6RqKJ0+eYOrUqQDePq9WVlaoXLmyynomJiZwdXUt9POak5OD0NBQ6ZgOGDAAx48fl/b/XWXLlsW8efMQFBSE5ORktdtLSUnBzp07pe35+vpi+/btSE1NLVQ9xZH7Hso1ffp0pdeAubm59Af3xo0bcHV1hZHR/30h/LfffqvUN699VCf3+Xp3UnddyodU0HtKk8+aGzduwNLSUvqPFPA2AL27v++elgoLC0ONGjXg7u4OQ0NDDBgwQCn0WlpaIiQkBF9//TX+/PNPhISE6F3AzO89mGv9+vUYMGAADA0NUbduXbi6umLHjh3S8hs3buT7+ZvbR1+9+7q2sLDA3r17sW3bNqXPwoEDB6q89uPj45W24+joCHNzc1hbW2PLli3o2bMnateunedj5U7e3t4fZD/zwrCjA0IIpTe7JqMV3bt3x40bN7Bu3bo8t10a+Pj44NGjR9i7d690cWejRo2kC2CvX7+OM2fOYODAgQDefpj3799f7cjBtm3bEBMTI01ubm7Izs7Gxo0blS5W9vX1RWhoqHRxa6dOneDo6CgFoPDwcMTHx3/wH4ht164dYmJicPr0aQwbNgwjRoyAj4+P1h/n0KFDSEtLQ9euXQG8Hd3KveBXHT8/P9ja2mLRokVql2/duhXVqlVD/fr1AQANGjSAs7Mztm3bpvXa3/f+e2jq1KlKr4GYmBg0adIkz/U///xzxMTEYN26dUhLS9PofZP7fL075XWx+4dU0HsKKPznw/thpEuXLoiJicHvv/+OtLQ0pQuLN2zYoPI+27FjB16+fCm1tW/fHs2bN8eQIUPg7OxcxD3UnYLegy9evMCuXbtU9vP9z6OCjq8+j3C8+7o+c+YMunTpAm9vb2k0HACWLVum8tp3cHBQ2s6xY8dw7tw5hIaGombNmli7dm2+j5U7/fjjjzrfx/zI5rex9Mm1a9dQtWpV1KxZU5pXd+HitWvX4ObmptQ2ZMgQ9OzZE59//jmEEAgMDAQApW01bNhQ7bZy++gLMzMzdOrUCZ06dcKcOXMwcuRIzJs3D8OHD8f69euRlZWl9EYSQsDU1BTff/89rKyspHYnJydUr15dadt//PEHHj58iP79+yu1Z2dnIzw8HJ06dYKBgQGGDx+OjRs3Yv78+QgJCUG7du3g6uqq2x1/T7ly5aT6N2zYgPr162P9+vXw8/NDzZo1kZycjEePHql8qLx58wa3b99Gu3btCvU469evx7Nnz1CmTBmpLScnB7GxsViwYIHKaJaRkRG++uorDB8+XO3vyq1fvx5XrlxRGjHJycnBhg0b4OfnV+j9L4rc91CuChUqqLwGctWoUQPHjx9HZmYmjI2NAby9s8za2hoPHjzQ+LHffb70TV7vqdw7LwvzWVOjRg0kJycjMTFRGt0xNzdH9erVlZ5rALh69SpOnTqFM2fOYPr06VJ7dnY2wsLCMGrUKKnNyMhIZX19kd97EHh7iio9PR3NmjWT1hFCICcnBzdu3EDNmjVRo0YNXLt2Te32c9v17TP4Xe+/rn/88UdYWVnhv//9L7788ksAgL29fYGv/apVq8La2hq1atXCkydP1N7Bp+49VJT3ojZxZEfLjhw5gkuXLsHHxwedO3eGjY0Nli5dqtJv7969uHnzpjSy8a5hw4YhNDQU06ZNwzfffAPg7f+qa9eujWXLlqnc2n7x4kUcPnxY7bb0iZubG9LS0pCVlYWffvoJS5cuVUr+Fy9ehIODQ6GuUckdcn7/fw/vD7GPGDEC9+/fx65du7B7926d/5EuiIGBAWbNmoXZs2fj9evX8PHxgbGxsdrXyNq1a5GWllao5zUpKQm//vorwsLClI7HhQsX8Pz5c/z5559q1/vss8/g7u6OBQsWKLVfunQJZ8+eRWRkpNL2IiMjER0djb///rtoB6AQ3n0PFcbAgQORmpqK1atX66wmfZX7ntLks6Zv374wNjbOc0TvXevXr0ebNm1w8eJFpddBYGDgB7l+Sxfefw8Cb/dz8uTJKp9Hn3zyiTQyOnDgQNy8eRO//fabyjaXLl0KBwcHdOrU6YPuS3EoFAoYGBiovWO1sPz9/XH58mXs3r1bi5XpyAe5DFqGhg0bJry8vERCQoJ48OCBOHfunPjqq6+Eubm56N69u8jKyhJCCLFjxw5haGgoRo0aJS5evCji4uLEjz/+KMqXLy/69u2rdOcD3rsa/ueffxaGhoZi8eLFQgghTpw4IcqWLSt69+4tTp8+Le7duye2b98unJycRMuWLZXurHF2dhZTpkwRFy5cUJqePXum82Pzzz//iHbt2olNmzaJixcvijt37ojt27eLSpUqic8//1zs3r1bmJiYiBcvXqisO23aNNGkSRMhRN53Yz158kQYGxtLd1S9648//hCmpqYiKSlJauvQoYMoX768sLa2Fq9fv1bqXxJ3Y2VmZooqVaqIJUuWCCGEWLZsmTAwMBCzZs0S165dE7du3RJLly4VpqamYvLkyWq3+/4dSsuWLROVK1dWeydNv379RN++fYUQ6vc3PDxcGBkZCSMjI+lurIkTJ4pmzZqpfeymTZuKKVOmFPII5K2w7yFnZ2excOFCkZCQoDQlJydL25o8ebIwNDQUAQEB4tixY+Lu3bsiOjpa+Pr6CoVCIfUNCQkR5ubmKu+Lq1evqtT07vT06VO19X+ou7EKek8JodlnzYoVK4RCoRBDhw4VR44cEXFxceLcuXMiICBAABCxsbHizZs3ws7OTqxZs0alnqtXrwoA4vLly1Jb27ZtxcSJE3V+LAqjoLux3n0PXrhwQQAQ165dU9nO6tWrhb29vcjMzBQ5OTmid+/eonz58uLHH38UcXFx4uLFi2L06NHCxMREHDlyRFpPH+/Gevd1ffXqVTF27FihUChERESEEOLt35+QkBCV137uHWZ5fVZOmzZNeHh4SK8vfb0bi2GniIYNGyYACADCyMhI2NnZiY4dO4oNGzao3BoeFRUlunTpIiwtLYWJiYlwd3cX33zzjfRhnuv9sCOEEFu2bBGGhobSbeWxsbHCx8dH2NjYCGNjY1GtWjUxe/ZskZaWprSes7OzVN+70/u3YutCenq6mDFjhmjUqJGwsrISZcuWFbVq1RKzZ88Wr169Et27dxddu3ZVu+7p06elWxnzCjvffPONsLa2Fm/evFFZPyMjQ1hbWyvd+rplyxYBQIwdO1alf0mEHSGECAoKEnZ2dtIHya+//io++eQTUa5cOWFmZiYaN24sNmzYkOd23w87Hh4eavdPCCG2bdsmTExMxNOnT/Pc386dO0sfdhkZGcLW1lYK2e9btGiRqFixotrjr4nCvofyei2PGTNGZT89PT2FlZWVMDY2Fo6OjmLQoEFKt+6HhISo3Va1atVUanp3qlWrltr6P1TYKeg9lauwnzVCCHHo0CHh7e0tbGxshJGRkahUqZLo3bu3OHDggBBCiJ07dwoDAwORmJiotqY6deoo3b5emsKOEP/3Hhw5cqRwc3NTu52EhARhYGAgfv31VyHE25C0ZMkS4e7uLkxMTAQAYWNjI65cuaK0nj6GnXdfzxYWFuLjjz8WO3fulPqoe90DEEFBQUKIvD8r4+PjhZGRkdi2bZv0WPoYdhRClJIrXomIiPTI+fPn0bFjR/j5+Wn9G8ZJu3jNDhERURE0atQI4eHhKFeuHG7fvl3S5VA+OLJDREREssaRHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIir1FAoF9uzZU9JlEJGeYtghIr2XmJiI8ePHw9XVFaampnByckKPHj0QHh5e0qURUSmgnz9RS0T0/929exetWrWCtbU1lixZAg8PD2RmZuLgwYPw9/fX6Y+SEpE8cGSHiPTa2LFjoVAocObMGfj4+KBmzZpwd3dHYGAgTp06pXad6dOno2bNmihbtixcXV0xZ84cZGZmSssvXryIdu3awcLCApaWlmjcuDHOnj0LALh37x569OiB8uXLo1y5cnB3d8cff/zxQfaViHSDIztEpLeePXuGAwcO4KuvvkK5cuVUlltbW6tdz8LCAqGhoXBwcMClS5cwatQoWFhYYNq0aQCAwYMHo2HDhlizZg0MDQ0RExMDY2NjAIC/vz/evHmDqKgolCtXDlevXoW5ubnO9pGIdI9hh4j01q1btyCEQO3atTVab/bs2dK/XVxcMGXKFISFhUlhJz4+HlOnTpW2W6NGDal/fHw8fHx84OHhAQBwdXUt7m4QUQnjaSwi0ltF/em+bdu2oVWrVrC3t4e5uTlmz56N+Ph4aXlgYCBGjhyJjh07Ijg4WOlHHCdMmIAvv/wSrVq1wrx58xAbG1vs/SCiksWwQ0R6q0aNGlAoFBpdhBwdHY3Bgweja9eu2LdvHy5cuIB///vfePPmjdRn/vz5uHLlCrp164YjR47Azc0Nu3fvBgCMHDkSd+7cwZAhQ3Dp0iU0adIEK1eu1Pq+EdGHw189JyK95u3tjUuXLuH69esq1+28ePEC1tbWUCgU2L17N3r37o2lS5di9erVSqM1I0eOxM6dO/HixQu1jzFw4ECkpaVh7969KstmzpyJ33//nSM8RKUYR3aISK+tWrUK2dnZaNq0KX755RfcvHkT165dw4oVK9CiRQuV/jVq1EB8fDzCwsJw+/ZtrFixQhq1AYDXr19j3LhxiIyMxL1793DixAn89ddfqFOnDgBg0qRJOHjwIOLi4nD+/HlERERIy4iodOIFykSk11xdXXH+/Hl89dVXmDx5MhISEmBnZ4fGjRtjzZo1Kv179uyJgIAAjBs3DhkZGejWrRvmzJmD+fPnAwAMDQ2RlJSEoUOH4vHjx6hQoQL69OmDBQsWAACys7Ph7++PBw8ewNLSEl5eXli2bNmH3GUi0jKexiIiIiJZ42ksIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKStf8HEBnYReER8WkAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Count the occurrences of each class\n",
        "class_counts = cd['Class'].value_counts()\n",
        "# Plot the data\n",
        "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
        "plt.title('Number of Elements in Each Class')\n",
        "plt.ylabel('Number of Elements')\n",
        "plt.xlabel('Class')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo4TzsRWL3KO"
      },
      "source": [
        "this dataset is imbalanced. As we can see a huge gap in the samples  between ( DOKOL, SAFAVi, ROTANA) and (DEGLET, SOGAY, IRAQI, BERHI)\n",
        "\n",
        "The total number of samples is 898, and there are 7 classes. So the average number of samples per class would be\n",
        "898/7= 128.29≈128. while DOKOL has around 200 samples, BERHI has just 65.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WPSitF8RvwK"
      },
      "source": [
        "# 1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mz8CupWpye52"
      },
      "outputs": [],
      "source": [
        "# Separating the labels into a separate DataFrame\n",
        "labels_df = pd.DataFrame(cd['Class'], columns=['Class'])\n",
        "\n",
        "# convert labels to integers\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels_df[\"Class\"])\n",
        "\n",
        "# RESHAPE THE LABELS INTO A 2D ARRAY\n",
        "encoded_labels = labels_encoded.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO-Oo-S7SIKi"
      },
      "source": [
        "# 1E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3DxnwQKye52",
        "outputId": "a4674a75-ca6e-416e-cb8f-74bd532b5877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (628, 34), Labels shape: (628, 1)\n",
            "Validation data shape: (135, 34), Labels shape: (135, 1)\n",
            "Testing data shape: (135, 34), Labels shape: (135, 1)\n"
          ]
        }
      ],
      "source": [
        "X = cd.drop(columns=\"Class\")\n",
        "y = encoded_labels\n",
        "\n",
        "# training/ Testing/ Validation - 70/15/15 respectively\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
        "\n",
        "# Print shapes of the datasets\n",
        "print(f\"Training data shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}, Labels shape: {y_val.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}, Labels shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sM2s6P70ye52"
      },
      "outputs": [],
      "source": [
        "# Scaling all the features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiQ7-J1Cye52",
        "outputId": "fa32a3d6-307b-4736-9108-18991b660ba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(628, 34)\n",
            "(135, 34)\n",
            "(135, 34)\n"
          ]
        }
      ],
      "source": [
        "print(X_train_scaled.shape)\n",
        "print(X_val_scaled.shape)\n",
        "print(X_test_scaled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YESUrRL7ye52",
        "outputId": "af0333ee-6894-4bc5-ddca-28564c720803"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.columns.value_counts().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2wZSn-pSRS4"
      },
      "source": [
        "# 2A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt2vGy2sye53",
        "outputId": "5489abee-4cf2-4fba-f27c-195ca4bd045e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               4480      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 7)                 455       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13191 (51.53 KB)\n",
            "Trainable params: 13191 (51.53 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Sequential model\n",
        "model = keras.Sequential([\n",
        "    # 34 features as an input\n",
        "    keras.layers.Input(34),\n",
        "\n",
        "    # First layer\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    # Second layer\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "\n",
        "    # Output layer with only 7 classes\n",
        "    keras.layers.Dense(7, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkWbgsuASVcc"
      },
      "source": [
        "# 2B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "W3lZttHiye53"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUm2pN1fSZB8"
      },
      "source": [
        "# 2C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXpXikWxye53",
        "outputId": "95bf1cda-e0f4-4e3a-b62e-8d313f9b3106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1759 - accuracy: 0.9299 - val_loss: 0.2017 - val_accuracy: 0.9185\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1694 - accuracy: 0.9443 - val_loss: 0.2212 - val_accuracy: 0.9111\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1766 - accuracy: 0.9299 - val_loss: 0.2211 - val_accuracy: 0.9185\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1673 - accuracy: 0.9363 - val_loss: 0.2191 - val_accuracy: 0.9037\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1587 - accuracy: 0.9411 - val_loss: 0.1948 - val_accuracy: 0.9185\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.1761 - accuracy: 0.9395 - val_loss: 0.2209 - val_accuracy: 0.9037\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.1665 - accuracy: 0.9379 - val_loss: 0.2240 - val_accuracy: 0.9037\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.1606 - accuracy: 0.9443 - val_loss: 0.2072 - val_accuracy: 0.9185\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1607 - accuracy: 0.9395 - val_loss: 0.2063 - val_accuracy: 0.9259\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1646 - accuracy: 0.9427 - val_loss: 0.2278 - val_accuracy: 0.8963\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1706 - accuracy: 0.9379 - val_loss: 0.2472 - val_accuracy: 0.8963\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1632 - accuracy: 0.9363 - val_loss: 0.1858 - val_accuracy: 0.9185\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.1682 - accuracy: 0.9347 - val_loss: 0.2238 - val_accuracy: 0.9037\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1557 - accuracy: 0.9475 - val_loss: 0.2371 - val_accuracy: 0.9037\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1719 - accuracy: 0.9299 - val_loss: 0.1997 - val_accuracy: 0.9037\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.1757 - accuracy: 0.9283 - val_loss: 0.2014 - val_accuracy: 0.9185\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1675 - accuracy: 0.9268 - val_loss: 0.2207 - val_accuracy: 0.9185\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1913 - accuracy: 0.9252 - val_loss: 0.2641 - val_accuracy: 0.8815\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1804 - accuracy: 0.9283 - val_loss: 0.2467 - val_accuracy: 0.8963\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1524 - accuracy: 0.9427 - val_loss: 0.1949 - val_accuracy: 0.9185\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1620 - accuracy: 0.9459 - val_loss: 0.1818 - val_accuracy: 0.9333\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1655 - accuracy: 0.9427 - val_loss: 0.1888 - val_accuracy: 0.9185\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.1453 - accuracy: 0.9427 - val_loss: 0.1977 - val_accuracy: 0.9111\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1482 - accuracy: 0.9475 - val_loss: 0.2419 - val_accuracy: 0.8963\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.1581 - accuracy: 0.9395 - val_loss: 0.2492 - val_accuracy: 0.9037\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1521 - accuracy: 0.9395 - val_loss: 0.1902 - val_accuracy: 0.9185\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1550 - accuracy: 0.9427 - val_loss: 0.2263 - val_accuracy: 0.8963\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1615 - accuracy: 0.9411 - val_loss: 0.2265 - val_accuracy: 0.9111\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1555 - accuracy: 0.9379 - val_loss: 0.2401 - val_accuracy: 0.8889\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1530 - accuracy: 0.9395 - val_loss: 0.1813 - val_accuracy: 0.9333\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1546 - accuracy: 0.9347 - val_loss: 0.2189 - val_accuracy: 0.8963\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1525 - accuracy: 0.9443 - val_loss: 0.1988 - val_accuracy: 0.9037\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1445 - accuracy: 0.9459 - val_loss: 0.2113 - val_accuracy: 0.9111\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1498 - accuracy: 0.9363 - val_loss: 0.2183 - val_accuracy: 0.9037\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1501 - accuracy: 0.9443 - val_loss: 0.2055 - val_accuracy: 0.8963\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.1523 - accuracy: 0.9443 - val_loss: 0.2025 - val_accuracy: 0.9185\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1464 - accuracy: 0.9395 - val_loss: 0.2139 - val_accuracy: 0.9185\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1452 - accuracy: 0.9475 - val_loss: 0.2224 - val_accuracy: 0.8963\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1475 - accuracy: 0.9506 - val_loss: 0.2616 - val_accuracy: 0.8889\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1640 - accuracy: 0.9395 - val_loss: 0.1985 - val_accuracy: 0.9259\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1468 - accuracy: 0.9395 - val_loss: 0.2014 - val_accuracy: 0.9259\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.1379 - accuracy: 0.9475 - val_loss: 0.2065 - val_accuracy: 0.9111\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1380 - accuracy: 0.9443 - val_loss: 0.2152 - val_accuracy: 0.9111\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.1350 - accuracy: 0.9506 - val_loss: 0.2016 - val_accuracy: 0.9259\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.1400 - accuracy: 0.9427 - val_loss: 0.2131 - val_accuracy: 0.9037\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1425 - accuracy: 0.9443 - val_loss: 0.1897 - val_accuracy: 0.9259\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1411 - accuracy: 0.9475 - val_loss: 0.1918 - val_accuracy: 0.9185\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1414 - accuracy: 0.9459 - val_loss: 0.2035 - val_accuracy: 0.9111\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1323 - accuracy: 0.9522 - val_loss: 0.1898 - val_accuracy: 0.9111\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1389 - accuracy: 0.9538 - val_loss: 0.2098 - val_accuracy: 0.9259\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1396 - accuracy: 0.9427 - val_loss: 0.2314 - val_accuracy: 0.9111\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1338 - accuracy: 0.9490 - val_loss: 0.1992 - val_accuracy: 0.9111\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1343 - accuracy: 0.9538 - val_loss: 0.1985 - val_accuracy: 0.9111\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1560 - accuracy: 0.9427 - val_loss: 0.1846 - val_accuracy: 0.9333\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1706 - accuracy: 0.9347 - val_loss: 0.1938 - val_accuracy: 0.9407\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1414 - accuracy: 0.9475 - val_loss: 0.2031 - val_accuracy: 0.9111\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1365 - accuracy: 0.9427 - val_loss: 0.2223 - val_accuracy: 0.9259\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1276 - accuracy: 0.9490 - val_loss: 0.2119 - val_accuracy: 0.9111\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1444 - accuracy: 0.9427 - val_loss: 0.1922 - val_accuracy: 0.9407\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1361 - accuracy: 0.9475 - val_loss: 0.1911 - val_accuracy: 0.9259\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.1354 - accuracy: 0.9522 - val_loss: 0.1883 - val_accuracy: 0.9259\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1343 - accuracy: 0.9443 - val_loss: 0.2028 - val_accuracy: 0.9185\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1480 - accuracy: 0.9506 - val_loss: 0.2256 - val_accuracy: 0.9037\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1318 - accuracy: 0.9538 - val_loss: 0.1820 - val_accuracy: 0.9185\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1267 - accuracy: 0.9602 - val_loss: 0.2037 - val_accuracy: 0.8963\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1484 - accuracy: 0.9395 - val_loss: 0.2194 - val_accuracy: 0.9037\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1501 - accuracy: 0.9395 - val_loss: 0.2514 - val_accuracy: 0.8889\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1495 - accuracy: 0.9459 - val_loss: 0.2412 - val_accuracy: 0.9037\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1329 - accuracy: 0.9522 - val_loss: 0.2256 - val_accuracy: 0.8889\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1270 - accuracy: 0.9506 - val_loss: 0.1937 - val_accuracy: 0.9185\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1253 - accuracy: 0.9538 - val_loss: 0.1938 - val_accuracy: 0.9259\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1259 - accuracy: 0.9602 - val_loss: 0.1976 - val_accuracy: 0.9111\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1217 - accuracy: 0.9554 - val_loss: 0.2102 - val_accuracy: 0.8963\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1172 - accuracy: 0.9506 - val_loss: 0.2009 - val_accuracy: 0.8963\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1326 - accuracy: 0.9522 - val_loss: 0.1863 - val_accuracy: 0.9185\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1362 - accuracy: 0.9459 - val_loss: 0.1977 - val_accuracy: 0.9037\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1229 - accuracy: 0.9570 - val_loss: 0.2100 - val_accuracy: 0.8963\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1288 - accuracy: 0.9506 - val_loss: 0.1846 - val_accuracy: 0.8963\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1212 - accuracy: 0.9538 - val_loss: 0.1960 - val_accuracy: 0.9185\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.1240 - accuracy: 0.9570 - val_loss: 0.1830 - val_accuracy: 0.9333\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.1265 - accuracy: 0.9586 - val_loss: 0.1907 - val_accuracy: 0.9333\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1288 - accuracy: 0.9490 - val_loss: 0.1980 - val_accuracy: 0.9259\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1323 - accuracy: 0.9490 - val_loss: 0.1898 - val_accuracy: 0.8963\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1333 - accuracy: 0.9490 - val_loss: 0.2045 - val_accuracy: 0.9185\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1260 - accuracy: 0.9538 - val_loss: 0.1924 - val_accuracy: 0.9037\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1158 - accuracy: 0.9602 - val_loss: 0.1977 - val_accuracy: 0.9185\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1153 - accuracy: 0.9602 - val_loss: 0.2067 - val_accuracy: 0.9111\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1219 - accuracy: 0.9634 - val_loss: 0.1873 - val_accuracy: 0.9111\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1142 - accuracy: 0.9618 - val_loss: 0.1991 - val_accuracy: 0.9185\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1187 - accuracy: 0.9570 - val_loss: 0.2268 - val_accuracy: 0.9111\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1132 - accuracy: 0.9586 - val_loss: 0.2108 - val_accuracy: 0.9185\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1208 - accuracy: 0.9554 - val_loss: 0.1846 - val_accuracy: 0.9111\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.1277 - accuracy: 0.9554 - val_loss: 0.2167 - val_accuracy: 0.9259\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1189 - accuracy: 0.9538 - val_loss: 0.2227 - val_accuracy: 0.8815\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1226 - accuracy: 0.9538 - val_loss: 0.2053 - val_accuracy: 0.9185\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1150 - accuracy: 0.9586 - val_loss: 0.2246 - val_accuracy: 0.8963\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9602 - val_loss: 0.1945 - val_accuracy: 0.9037\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1147 - accuracy: 0.9586 - val_loss: 0.1982 - val_accuracy: 0.9111\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1108 - accuracy: 0.9570 - val_loss: 0.1898 - val_accuracy: 0.9259\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1250 - accuracy: 0.9554 - val_loss: 0.2348 - val_accuracy: 0.9185\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.2348 - accuracy: 0.9185\n",
            "Validation Accuracy: 0.9185\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "Confusion Matrix:\n",
            "[[13  0  0  0  0  0  0]\n",
            " [ 0 14  3  0  0  0  4]\n",
            " [ 0  0 25  0  0  0  0]\n",
            " [ 1  0  0  7  0  0  0]\n",
            " [ 0  0  0  0 27  0  1]\n",
            " [ 0  0  0  0  0 26  0]\n",
            " [ 1  1  0  0  0  0 12]]\n"
          ]
        }
      ],
      "source": [
        "# Train the model the model named history\n",
        "history = model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_val_scaled, y_val))\n",
        "\n",
        "# Record training accuracy\n",
        "training_accuracy = history.history['accuracy'][-1]\n",
        "\n",
        "# Evaluate the model on the validation data to get validation accuracy\n",
        "validation_loss, validation_accuracy = model.evaluate(X_val_scaled, y_val)\n",
        "print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n",
        "\n",
        "# Predict labels on the validation data\n",
        "y_val_pred = model.predict(X_val_scaled)\n",
        "y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_val, y_val_pred_labels)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzqwt7oLye53",
        "outputId": "b64537a6-fa70-4818-d058-f2b2e64381b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[13  0  0  0  0  0  0]\n",
            " [ 0 14  3  0  0  0  4]\n",
            " [ 0  0 25  0  0  0  0]\n",
            " [ 1  0  0  7  0  0  0]\n",
            " [ 0  0  0  0 27  0  1]\n",
            " [ 0  0  0  0  0 26  0]\n",
            " [ 1  1  0  0  0  0 12]]\n",
            "\n",
            "Validation Accuracy: 0.9185\n"
          ]
        }
      ],
      "source": [
        "# Printing Confusion Matrix and Validation Accuracy.\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\")\n",
        "print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The validation accuracy of approximately 91.85% indicates that the model is making correct predictions for a majority of the samples. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNHF9pbBSlqi"
      },
      "source": [
        "# 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rORy3BmkWOX7"
      },
      "outputs": [],
      "source": [
        "# Initializing lists to store results\n",
        "model_descriptions = []\n",
        "training_accuracies = []\n",
        "validation_accuracies = []\n",
        "confusion_matrices = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2PIAW1YWlKj",
        "outputId": "15191391-204c-4d9c-d936-d6e1c9db6114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 2s 29ms/step - loss: 1.6423 - accuracy: 0.5494 - val_loss: 1.4668 - val_accuracy: 0.5630\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 1.1314 - accuracy: 0.6322 - val_loss: 1.0290 - val_accuracy: 0.6222\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.8016 - accuracy: 0.6943 - val_loss: 0.7690 - val_accuracy: 0.6667\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.6428 - accuracy: 0.7611 - val_loss: 0.6397 - val_accuracy: 0.7407\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.5241 - accuracy: 0.7994 - val_loss: 0.5441 - val_accuracy: 0.7926\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.4805 - accuracy: 0.8248 - val_loss: 0.5216 - val_accuracy: 0.8148\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.4473 - accuracy: 0.8392 - val_loss: 0.5068 - val_accuracy: 0.8148\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.4259 - accuracy: 0.8280 - val_loss: 0.4465 - val_accuracy: 0.8000\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.3844 - accuracy: 0.8424 - val_loss: 0.4597 - val_accuracy: 0.8000\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.3855 - accuracy: 0.8615 - val_loss: 0.4373 - val_accuracy: 0.7852\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.3730 - accuracy: 0.8408 - val_loss: 0.4080 - val_accuracy: 0.8370\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.3390 - accuracy: 0.8758 - val_loss: 0.3692 - val_accuracy: 0.8815\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.3312 - accuracy: 0.8838 - val_loss: 0.4248 - val_accuracy: 0.8593\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.3280 - accuracy: 0.8790 - val_loss: 0.3999 - val_accuracy: 0.8667\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.3415 - accuracy: 0.8694 - val_loss: 0.4081 - val_accuracy: 0.8296\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3473 - accuracy: 0.8583 - val_loss: 0.3992 - val_accuracy: 0.8222\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3356 - accuracy: 0.8758 - val_loss: 0.3579 - val_accuracy: 0.8370\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.3013 - accuracy: 0.8869 - val_loss: 0.3505 - val_accuracy: 0.8370\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.2854 - accuracy: 0.8838 - val_loss: 0.3751 - val_accuracy: 0.8370\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.3190 - accuracy: 0.8662 - val_loss: 0.3787 - val_accuracy: 0.8444\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.3038 - accuracy: 0.8694 - val_loss: 0.4046 - val_accuracy: 0.8370\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.3181 - accuracy: 0.8869 - val_loss: 0.3914 - val_accuracy: 0.8741\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.3092 - accuracy: 0.8901 - val_loss: 0.4042 - val_accuracy: 0.8519\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2900 - accuracy: 0.8917 - val_loss: 0.3296 - val_accuracy: 0.8963\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2632 - accuracy: 0.9029 - val_loss: 0.3053 - val_accuracy: 0.9259\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2441 - accuracy: 0.9076 - val_loss: 0.3612 - val_accuracy: 0.8741\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2492 - accuracy: 0.9076 - val_loss: 0.3186 - val_accuracy: 0.9037\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2517 - accuracy: 0.9092 - val_loss: 0.3091 - val_accuracy: 0.8815\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2303 - accuracy: 0.9236 - val_loss: 0.2914 - val_accuracy: 0.8889\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2438 - accuracy: 0.9013 - val_loss: 0.2954 - val_accuracy: 0.8963\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2458 - accuracy: 0.8997 - val_loss: 0.2979 - val_accuracy: 0.8667\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2498 - accuracy: 0.9013 - val_loss: 0.3023 - val_accuracy: 0.8963\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2294 - accuracy: 0.9061 - val_loss: 0.2776 - val_accuracy: 0.8815\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2202 - accuracy: 0.9236 - val_loss: 0.2796 - val_accuracy: 0.8741\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2231 - accuracy: 0.9172 - val_loss: 0.3349 - val_accuracy: 0.8593\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.2451 - accuracy: 0.8965 - val_loss: 0.2927 - val_accuracy: 0.8889\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2253 - accuracy: 0.9172 - val_loss: 0.3256 - val_accuracy: 0.8593\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2680 - accuracy: 0.8885 - val_loss: 0.3615 - val_accuracy: 0.8296\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2422 - accuracy: 0.9029 - val_loss: 0.2637 - val_accuracy: 0.9259\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.2093 - accuracy: 0.9204 - val_loss: 0.2572 - val_accuracy: 0.8889\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2136 - accuracy: 0.9172 - val_loss: 0.2720 - val_accuracy: 0.9333\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2065 - accuracy: 0.9108 - val_loss: 0.2456 - val_accuracy: 0.9259\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2303 - accuracy: 0.9061 - val_loss: 0.2820 - val_accuracy: 0.8815\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2144 - accuracy: 0.9140 - val_loss: 0.3305 - val_accuracy: 0.8444\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2090 - accuracy: 0.9268 - val_loss: 0.3349 - val_accuracy: 0.8741\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.1982 - accuracy: 0.9299 - val_loss: 0.2694 - val_accuracy: 0.8667\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1965 - accuracy: 0.9283 - val_loss: 0.2391 - val_accuracy: 0.9111\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1878 - accuracy: 0.9172 - val_loss: 0.2618 - val_accuracy: 0.9111\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 18ms/step - loss: 0.1991 - accuracy: 0.9188 - val_loss: 0.3200 - val_accuracy: 0.9037\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.2124 - accuracy: 0.9220 - val_loss: 0.2575 - val_accuracy: 0.8889\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.2037 - accuracy: 0.9220 - val_loss: 0.2472 - val_accuracy: 0.8963\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1927 - accuracy: 0.9220 - val_loss: 0.2436 - val_accuracy: 0.9037\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1919 - accuracy: 0.9299 - val_loss: 0.2510 - val_accuracy: 0.9111\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.1847 - accuracy: 0.9268 - val_loss: 0.2404 - val_accuracy: 0.9111\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1632 - accuracy: 0.9443 - val_loss: 0.2543 - val_accuracy: 0.9111\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1749 - accuracy: 0.9315 - val_loss: 0.2489 - val_accuracy: 0.9037\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1786 - accuracy: 0.9315 - val_loss: 0.2780 - val_accuracy: 0.8815\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2035 - accuracy: 0.9092 - val_loss: 0.2385 - val_accuracy: 0.9037\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2010 - accuracy: 0.9268 - val_loss: 0.2554 - val_accuracy: 0.9185\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1651 - accuracy: 0.9427 - val_loss: 0.2534 - val_accuracy: 0.8963\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.1642 - accuracy: 0.9331 - val_loss: 0.2966 - val_accuracy: 0.8815\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.1857 - accuracy: 0.9268 - val_loss: 0.2737 - val_accuracy: 0.8741\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1872 - accuracy: 0.9220 - val_loss: 0.4563 - val_accuracy: 0.8296\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.2184 - accuracy: 0.9140 - val_loss: 0.3570 - val_accuracy: 0.8741\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1896 - accuracy: 0.9188 - val_loss: 0.3390 - val_accuracy: 0.8593\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1873 - accuracy: 0.9315 - val_loss: 0.3058 - val_accuracy: 0.8519\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1707 - accuracy: 0.9283 - val_loss: 0.2858 - val_accuracy: 0.8889\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1723 - accuracy: 0.9315 - val_loss: 0.2675 - val_accuracy: 0.8815\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1760 - accuracy: 0.9299 - val_loss: 0.2969 - val_accuracy: 0.8889\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2022 - accuracy: 0.9172 - val_loss: 0.3866 - val_accuracy: 0.8593\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2146 - accuracy: 0.9092 - val_loss: 0.2558 - val_accuracy: 0.9037\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1929 - accuracy: 0.9268 - val_loss: 0.2440 - val_accuracy: 0.9037\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.1742 - accuracy: 0.9283 - val_loss: 0.2518 - val_accuracy: 0.9333\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.1807 - accuracy: 0.9315 - val_loss: 0.2624 - val_accuracy: 0.9111\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1797 - accuracy: 0.9220 - val_loss: 0.2490 - val_accuracy: 0.8815\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.1700 - accuracy: 0.9379 - val_loss: 0.2291 - val_accuracy: 0.9111\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1794 - accuracy: 0.9299 - val_loss: 0.3066 - val_accuracy: 0.8815\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1587 - accuracy: 0.9331 - val_loss: 0.2571 - val_accuracy: 0.8815\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1495 - accuracy: 0.9379 - val_loss: 0.2762 - val_accuracy: 0.8519\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.1495 - accuracy: 0.9395 - val_loss: 0.3059 - val_accuracy: 0.8889\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.1597 - accuracy: 0.9268 - val_loss: 0.2483 - val_accuracy: 0.8741\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.1643 - accuracy: 0.9395 - val_loss: 0.2372 - val_accuracy: 0.8815\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1435 - accuracy: 0.9475 - val_loss: 0.3073 - val_accuracy: 0.8889\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.1627 - accuracy: 0.9363 - val_loss: 0.3134 - val_accuracy: 0.8815\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.1388 - accuracy: 0.9411 - val_loss: 0.2461 - val_accuracy: 0.8889\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.1444 - accuracy: 0.9443 - val_loss: 0.2852 - val_accuracy: 0.8963\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1536 - accuracy: 0.9427 - val_loss: 0.2360 - val_accuracy: 0.8963\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1282 - accuracy: 0.9427 - val_loss: 0.2232 - val_accuracy: 0.8963\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1259 - accuracy: 0.9490 - val_loss: 0.2170 - val_accuracy: 0.9037\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1174 - accuracy: 0.9602 - val_loss: 0.2865 - val_accuracy: 0.8963\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1556 - accuracy: 0.9347 - val_loss: 0.2886 - val_accuracy: 0.9185\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1252 - accuracy: 0.9538 - val_loss: 0.2292 - val_accuracy: 0.9111\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1226 - accuracy: 0.9506 - val_loss: 0.2253 - val_accuracy: 0.9037\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1309 - accuracy: 0.9443 - val_loss: 0.2858 - val_accuracy: 0.8815\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1237 - accuracy: 0.9490 - val_loss: 0.2479 - val_accuracy: 0.8741\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1495 - accuracy: 0.9427 - val_loss: 0.2248 - val_accuracy: 0.9037\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1399 - accuracy: 0.9459 - val_loss: 0.3139 - val_accuracy: 0.8963\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.1663 - accuracy: 0.9283 - val_loss: 0.2356 - val_accuracy: 0.8963\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1358 - accuracy: 0.9443 - val_loss: 0.2356 - val_accuracy: 0.8963\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.1268 - accuracy: 0.9554 - val_loss: 0.2705 - val_accuracy: 0.8815\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2705 - accuracy: 0.8815\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        }
      ],
      "source": [
        "# Model 1: Changing layer dimensions\n",
        "model_1 = keras.Sequential([\n",
        "    keras.layers.Input(34),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(7, activation='softmax')\n",
        "])\n",
        "# Compile the model\n",
        "\n",
        "model_1.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_1 = model_1.fit(X_train_scaled, y_train, epochs=100,\n",
        "                        validation_data=(X_val_scaled, y_val))\n",
        "\n",
        "model_descriptions.append(\"3 layers: 256, 128, 64 neurons. Adam optimizer. Relu activations.\")\n",
        "\n",
        "# Record training accuracy\n",
        "training_accuracies.append(history_1.history['accuracy'][-1])\n",
        "# Evaluate the model on the validation data to get validation accuracy\n",
        "\n",
        "validation_loss, validation_accuracy = model_1.evaluate(X_val_scaled, y_val)\n",
        "validation_accuracies.append(validation_accuracy)\n",
        "\n",
        "# Predict labels on the validation data\n",
        "\n",
        "y_val_pred = model_1.predict(X_val_scaled)\n",
        "y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "confusion_matrices.append(confusion_matrix(y_val, y_val_pred_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFQ1c6hYYsYj",
        "outputId": "a7755e75-f90d-4be9-fb41-a7591bd17b23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "20/20 [==============================] - 2s 16ms/step - loss: 1.6098 - accuracy: 0.5143 - val_loss: 1.4394 - val_accuracy: 0.5852\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1.1678 - accuracy: 0.6433 - val_loss: 1.1305 - val_accuracy: 0.5852\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.9124 - accuracy: 0.6879 - val_loss: 0.9228 - val_accuracy: 0.6593\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.7521 - accuracy: 0.7643 - val_loss: 0.7774 - val_accuracy: 0.7037\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.6406 - accuracy: 0.8089 - val_loss: 0.6731 - val_accuracy: 0.7556\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5663 - accuracy: 0.8376 - val_loss: 0.6018 - val_accuracy: 0.7407\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5181 - accuracy: 0.8280 - val_loss: 0.5919 - val_accuracy: 0.7778\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.4639 - accuracy: 0.8583 - val_loss: 0.5038 - val_accuracy: 0.8593\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4298 - accuracy: 0.8583 - val_loss: 0.4704 - val_accuracy: 0.8963\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.4083 - accuracy: 0.8742 - val_loss: 0.4566 - val_accuracy: 0.8593\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.3918 - accuracy: 0.8599 - val_loss: 0.4275 - val_accuracy: 0.8963\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.3681 - accuracy: 0.8790 - val_loss: 0.4078 - val_accuracy: 0.9037\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.3516 - accuracy: 0.8869 - val_loss: 0.3847 - val_accuracy: 0.9111\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.3407 - accuracy: 0.8949 - val_loss: 0.3824 - val_accuracy: 0.8963\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.3321 - accuracy: 0.8901 - val_loss: 0.3708 - val_accuracy: 0.9111\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.3184 - accuracy: 0.8965 - val_loss: 0.3812 - val_accuracy: 0.8815\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3084 - accuracy: 0.8949 - val_loss: 0.3346 - val_accuracy: 0.9111\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2957 - accuracy: 0.9108 - val_loss: 0.3355 - val_accuracy: 0.9037\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2922 - accuracy: 0.9061 - val_loss: 0.3178 - val_accuracy: 0.9111\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2789 - accuracy: 0.9045 - val_loss: 0.3135 - val_accuracy: 0.9185\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2767 - accuracy: 0.9124 - val_loss: 0.3211 - val_accuracy: 0.9111\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2787 - accuracy: 0.9092 - val_loss: 0.2970 - val_accuracy: 0.9259\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2636 - accuracy: 0.9156 - val_loss: 0.2973 - val_accuracy: 0.8963\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2654 - accuracy: 0.9124 - val_loss: 0.3109 - val_accuracy: 0.8963\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2588 - accuracy: 0.9108 - val_loss: 0.3085 - val_accuracy: 0.9111\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2608 - accuracy: 0.9076 - val_loss: 0.2896 - val_accuracy: 0.9037\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2621 - accuracy: 0.9061 - val_loss: 0.3349 - val_accuracy: 0.8593\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2583 - accuracy: 0.9108 - val_loss: 0.2978 - val_accuracy: 0.8889\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2409 - accuracy: 0.9220 - val_loss: 0.2591 - val_accuracy: 0.9111\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2373 - accuracy: 0.9283 - val_loss: 0.2560 - val_accuracy: 0.9259\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2465 - accuracy: 0.9061 - val_loss: 0.2829 - val_accuracy: 0.8963\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2391 - accuracy: 0.9108 - val_loss: 0.2760 - val_accuracy: 0.9037\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.2247 - accuracy: 0.9204 - val_loss: 0.2543 - val_accuracy: 0.9185\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2224 - accuracy: 0.9204 - val_loss: 0.2727 - val_accuracy: 0.9037\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.2201 - accuracy: 0.9252 - val_loss: 0.2445 - val_accuracy: 0.9185\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2180 - accuracy: 0.9252 - val_loss: 0.2649 - val_accuracy: 0.9111\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2091 - accuracy: 0.9315 - val_loss: 0.2604 - val_accuracy: 0.9111\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2324 - accuracy: 0.9140 - val_loss: 0.2776 - val_accuracy: 0.8815\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2231 - accuracy: 0.9204 - val_loss: 0.2385 - val_accuracy: 0.9111\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2110 - accuracy: 0.9299 - val_loss: 0.2511 - val_accuracy: 0.9111\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2122 - accuracy: 0.9236 - val_loss: 0.2578 - val_accuracy: 0.8815\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2124 - accuracy: 0.9283 - val_loss: 0.2896 - val_accuracy: 0.8667\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2175 - accuracy: 0.9172 - val_loss: 0.2513 - val_accuracy: 0.9037\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2009 - accuracy: 0.9299 - val_loss: 0.2322 - val_accuracy: 0.9185\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1987 - accuracy: 0.9299 - val_loss: 0.2192 - val_accuracy: 0.9111\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1978 - accuracy: 0.9379 - val_loss: 0.2385 - val_accuracy: 0.9111\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.1986 - accuracy: 0.9236 - val_loss: 0.2280 - val_accuracy: 0.9259\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2033 - accuracy: 0.9236 - val_loss: 0.2469 - val_accuracy: 0.9037\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1933 - accuracy: 0.9315 - val_loss: 0.2303 - val_accuracy: 0.9111\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1896 - accuracy: 0.9315 - val_loss: 0.2155 - val_accuracy: 0.9037\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2015 - accuracy: 0.9299 - val_loss: 0.2195 - val_accuracy: 0.9185\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1883 - accuracy: 0.9363 - val_loss: 0.2179 - val_accuracy: 0.9185\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1824 - accuracy: 0.9236 - val_loss: 0.2201 - val_accuracy: 0.9037\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1879 - accuracy: 0.9299 - val_loss: 0.2057 - val_accuracy: 0.9037\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1881 - accuracy: 0.9315 - val_loss: 0.2243 - val_accuracy: 0.9259\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1924 - accuracy: 0.9268 - val_loss: 0.2377 - val_accuracy: 0.9111\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1976 - accuracy: 0.9220 - val_loss: 0.2056 - val_accuracy: 0.8963\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1776 - accuracy: 0.9331 - val_loss: 0.2192 - val_accuracy: 0.9111\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1792 - accuracy: 0.9252 - val_loss: 0.2384 - val_accuracy: 0.9185\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1840 - accuracy: 0.9299 - val_loss: 0.2255 - val_accuracy: 0.9185\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.1842 - accuracy: 0.9315 - val_loss: 0.2086 - val_accuracy: 0.9185\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1755 - accuracy: 0.9315 - val_loss: 0.2093 - val_accuracy: 0.9037\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.1777 - accuracy: 0.9315 - val_loss: 0.2263 - val_accuracy: 0.9185\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.1745 - accuracy: 0.9347 - val_loss: 0.2324 - val_accuracy: 0.9111\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1720 - accuracy: 0.9411 - val_loss: 0.2045 - val_accuracy: 0.9185\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.1697 - accuracy: 0.9363 - val_loss: 0.1967 - val_accuracy: 0.9111\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1750 - accuracy: 0.9268 - val_loss: 0.2231 - val_accuracy: 0.9111\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1880 - accuracy: 0.9283 - val_loss: 0.2062 - val_accuracy: 0.9185\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1674 - accuracy: 0.9331 - val_loss: 0.2195 - val_accuracy: 0.9111\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1683 - accuracy: 0.9443 - val_loss: 0.2112 - val_accuracy: 0.9185\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.1631 - accuracy: 0.9411 - val_loss: 0.2512 - val_accuracy: 0.8741\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1794 - accuracy: 0.9347 - val_loss: 0.2545 - val_accuracy: 0.9037\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1751 - accuracy: 0.9427 - val_loss: 0.2019 - val_accuracy: 0.9259\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.1708 - accuracy: 0.9363 - val_loss: 0.1991 - val_accuracy: 0.9185\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.1647 - accuracy: 0.9411 - val_loss: 0.1960 - val_accuracy: 0.9333\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.1567 - accuracy: 0.9379 - val_loss: 0.1930 - val_accuracy: 0.9333\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.1565 - accuracy: 0.9427 - val_loss: 0.1914 - val_accuracy: 0.9259\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1519 - accuracy: 0.9395 - val_loss: 0.1913 - val_accuracy: 0.9111\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.1654 - accuracy: 0.9427 - val_loss: 0.2060 - val_accuracy: 0.9037\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.1702 - accuracy: 0.9283 - val_loss: 0.1945 - val_accuracy: 0.9333\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1771 - accuracy: 0.9299 - val_loss: 0.1810 - val_accuracy: 0.9111\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1593 - accuracy: 0.9395 - val_loss: 0.1845 - val_accuracy: 0.9259\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1490 - accuracy: 0.9459 - val_loss: 0.1911 - val_accuracy: 0.8963\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1559 - accuracy: 0.9411 - val_loss: 0.2049 - val_accuracy: 0.9185\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1562 - accuracy: 0.9459 - val_loss: 0.2129 - val_accuracy: 0.8963\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1520 - accuracy: 0.9427 - val_loss: 0.2031 - val_accuracy: 0.8963\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1498 - accuracy: 0.9459 - val_loss: 0.2088 - val_accuracy: 0.9111\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1569 - accuracy: 0.9283 - val_loss: 0.2760 - val_accuracy: 0.8889\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1802 - accuracy: 0.9204 - val_loss: 0.1752 - val_accuracy: 0.9037\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1501 - accuracy: 0.9522 - val_loss: 0.1840 - val_accuracy: 0.9185\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1557 - accuracy: 0.9379 - val_loss: 0.1884 - val_accuracy: 0.9556\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1819 - accuracy: 0.9252 - val_loss: 0.1866 - val_accuracy: 0.9333\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.1487 - accuracy: 0.9522 - val_loss: 0.1780 - val_accuracy: 0.9333\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1588 - accuracy: 0.9395 - val_loss: 0.1821 - val_accuracy: 0.9259\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1438 - accuracy: 0.9443 - val_loss: 0.1748 - val_accuracy: 0.9111\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1432 - accuracy: 0.9475 - val_loss: 0.1782 - val_accuracy: 0.9259\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.1420 - accuracy: 0.9490 - val_loss: 0.2109 - val_accuracy: 0.9111\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.1730 - accuracy: 0.9283 - val_loss: 0.1926 - val_accuracy: 0.9111\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1574 - accuracy: 0.9395 - val_loss: 0.2197 - val_accuracy: 0.9037\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.1487 - accuracy: 0.9475 - val_loss: 0.2321 - val_accuracy: 0.9037\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.2321 - accuracy: 0.9037\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# Model 2: Changing activation functions\n",
        "model_2 = keras.Sequential([\n",
        "    keras.layers.Input(34),\n",
        "    keras.layers.Dense(128, activation='tanh'),\n",
        "    keras.layers.Dense(64, activation='tanh'),\n",
        "    keras.layers.Dense(7, activation='softmax')\n",
        "])\n",
        "model_2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_2 = model_2.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_val_scaled, y_val))\n",
        "model_descriptions.append(\"2 layers: 128, 64 neurons. Adam optimizer. Tanh activations.\")\n",
        "training_accuracies.append(history_2.history['accuracy'][-1])\n",
        "validation_loss, validation_accuracy = model_2.evaluate(X_val_scaled, y_val)\n",
        "validation_accuracies.append(validation_accuracy)\n",
        "y_val_pred = model_2.predict(X_val_scaled)\n",
        "y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
        "confusion_matrices.append(confusion_matrix(y_val, y_val_pred_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh5rV-TvYx-7",
        "outputId": "83064c86-c49e-43fd-b5ba-ccfda6b18dec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "20/20 [==============================] - 1s 17ms/step - loss: 1.9661 - accuracy: 0.2070 - val_loss: 1.8362 - val_accuracy: 0.3778\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 1.7847 - accuracy: 0.3615 - val_loss: 1.6845 - val_accuracy: 0.3852\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 1.6390 - accuracy: 0.4682 - val_loss: 1.5365 - val_accuracy: 0.5333\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 1.4992 - accuracy: 0.5127 - val_loss: 1.4014 - val_accuracy: 0.5630\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 1.3826 - accuracy: 0.5589 - val_loss: 1.2808 - val_accuracy: 0.5852\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 1.2545 - accuracy: 0.5796 - val_loss: 1.1942 - val_accuracy: 0.5852\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 1.1799 - accuracy: 0.6162 - val_loss: 1.1081 - val_accuracy: 0.5852\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 1.1156 - accuracy: 0.6274 - val_loss: 1.0299 - val_accuracy: 0.5926\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 1.0815 - accuracy: 0.6226 - val_loss: 0.9761 - val_accuracy: 0.6074\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 1.0213 - accuracy: 0.6545 - val_loss: 0.8971 - val_accuracy: 0.6296\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.9340 - accuracy: 0.6624 - val_loss: 0.8325 - val_accuracy: 0.6296\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.8904 - accuracy: 0.6831 - val_loss: 0.8020 - val_accuracy: 0.6667\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.8909 - accuracy: 0.6720 - val_loss: 0.7602 - val_accuracy: 0.6815\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.8305 - accuracy: 0.7213 - val_loss: 0.7399 - val_accuracy: 0.6741\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.8305 - accuracy: 0.7006 - val_loss: 0.7006 - val_accuracy: 0.7407\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.7677 - accuracy: 0.7373 - val_loss: 0.6724 - val_accuracy: 0.7556\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 17ms/step - loss: 0.7577 - accuracy: 0.7277 - val_loss: 0.6542 - val_accuracy: 0.7333\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 17ms/step - loss: 0.7306 - accuracy: 0.7357 - val_loss: 0.6207 - val_accuracy: 0.7778\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.7162 - accuracy: 0.7341 - val_loss: 0.6061 - val_accuracy: 0.7630\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.6794 - accuracy: 0.7325 - val_loss: 0.5842 - val_accuracy: 0.8074\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.7050 - accuracy: 0.7373 - val_loss: 0.5745 - val_accuracy: 0.7778\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.6693 - accuracy: 0.7404 - val_loss: 0.5872 - val_accuracy: 0.7778\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.6504 - accuracy: 0.7420 - val_loss: 0.5570 - val_accuracy: 0.7630\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.6793 - accuracy: 0.7596 - val_loss: 0.5672 - val_accuracy: 0.7778\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.6242 - accuracy: 0.7532 - val_loss: 0.5346 - val_accuracy: 0.8222\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.6137 - accuracy: 0.7755 - val_loss: 0.5109 - val_accuracy: 0.7852\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.6056 - accuracy: 0.7787 - val_loss: 0.4968 - val_accuracy: 0.8222\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.5902 - accuracy: 0.7914 - val_loss: 0.5005 - val_accuracy: 0.8222\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.6183 - accuracy: 0.7564 - val_loss: 0.5056 - val_accuracy: 0.8296\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.5843 - accuracy: 0.7803 - val_loss: 0.4815 - val_accuracy: 0.8519\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.5657 - accuracy: 0.7898 - val_loss: 0.4824 - val_accuracy: 0.8000\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.5649 - accuracy: 0.8010 - val_loss: 0.4775 - val_accuracy: 0.8000\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.5646 - accuracy: 0.7866 - val_loss: 0.4594 - val_accuracy: 0.8444\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.5534 - accuracy: 0.7914 - val_loss: 0.4780 - val_accuracy: 0.8296\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5586 - accuracy: 0.7946 - val_loss: 0.4887 - val_accuracy: 0.7926\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5484 - accuracy: 0.7898 - val_loss: 0.4504 - val_accuracy: 0.8444\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.5134 - accuracy: 0.7914 - val_loss: 0.4524 - val_accuracy: 0.8444\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.5209 - accuracy: 0.7930 - val_loss: 0.4590 - val_accuracy: 0.8741\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.5206 - accuracy: 0.7994 - val_loss: 0.4411 - val_accuracy: 0.8370\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.5101 - accuracy: 0.8137 - val_loss: 0.4272 - val_accuracy: 0.8444\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.5136 - accuracy: 0.7978 - val_loss: 0.4318 - val_accuracy: 0.8593\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5149 - accuracy: 0.8041 - val_loss: 0.4067 - val_accuracy: 0.8963\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.4759 - accuracy: 0.8169 - val_loss: 0.4285 - val_accuracy: 0.8296\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5028 - accuracy: 0.7866 - val_loss: 0.4258 - val_accuracy: 0.8667\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4804 - accuracy: 0.8185 - val_loss: 0.4320 - val_accuracy: 0.8370\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.5107 - accuracy: 0.8041 - val_loss: 0.4155 - val_accuracy: 0.8741\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4906 - accuracy: 0.8217 - val_loss: 0.4188 - val_accuracy: 0.8519\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.4777 - accuracy: 0.8312 - val_loss: 0.4010 - val_accuracy: 0.8667\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4559 - accuracy: 0.8424 - val_loss: 0.4024 - val_accuracy: 0.8593\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4504 - accuracy: 0.8344 - val_loss: 0.3854 - val_accuracy: 0.8815\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.4673 - accuracy: 0.8137 - val_loss: 0.4000 - val_accuracy: 0.8444\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.4872 - accuracy: 0.8185 - val_loss: 0.3877 - val_accuracy: 0.8889\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4556 - accuracy: 0.8185 - val_loss: 0.3878 - val_accuracy: 0.8963\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.4442 - accuracy: 0.8360 - val_loss: 0.3760 - val_accuracy: 0.9037\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.4669 - accuracy: 0.8217 - val_loss: 0.4093 - val_accuracy: 0.8815\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.4696 - accuracy: 0.8360 - val_loss: 0.3893 - val_accuracy: 0.8889\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.4496 - accuracy: 0.8376 - val_loss: 0.3717 - val_accuracy: 0.8667\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.4337 - accuracy: 0.8360 - val_loss: 0.3868 - val_accuracy: 0.8889\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4851 - accuracy: 0.8185 - val_loss: 0.3850 - val_accuracy: 0.8815\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4519 - accuracy: 0.8185 - val_loss: 0.3766 - val_accuracy: 0.9037\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4531 - accuracy: 0.8296 - val_loss: 0.3797 - val_accuracy: 0.8889\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.4613 - accuracy: 0.8201 - val_loss: 0.3651 - val_accuracy: 0.8889\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4376 - accuracy: 0.8471 - val_loss: 0.3801 - val_accuracy: 0.8889\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4370 - accuracy: 0.8217 - val_loss: 0.3576 - val_accuracy: 0.9037\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.4108 - accuracy: 0.8567 - val_loss: 0.3692 - val_accuracy: 0.8815\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.4332 - accuracy: 0.8312 - val_loss: 0.3674 - val_accuracy: 0.8889\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4364 - accuracy: 0.8296 - val_loss: 0.3705 - val_accuracy: 0.8889\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.4161 - accuracy: 0.8471 - val_loss: 0.3646 - val_accuracy: 0.8889\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4437 - accuracy: 0.8535 - val_loss: 0.3575 - val_accuracy: 0.8889\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4070 - accuracy: 0.8599 - val_loss: 0.3708 - val_accuracy: 0.8889\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4586 - accuracy: 0.8248 - val_loss: 0.3546 - val_accuracy: 0.9037\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.4019 - accuracy: 0.8376 - val_loss: 0.3397 - val_accuracy: 0.9185\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3876 - accuracy: 0.8360 - val_loss: 0.3390 - val_accuracy: 0.9111\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.4075 - accuracy: 0.8424 - val_loss: 0.3530 - val_accuracy: 0.9037\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.4165 - accuracy: 0.8471 - val_loss: 0.3674 - val_accuracy: 0.8889\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3863 - accuracy: 0.8503 - val_loss: 0.3336 - val_accuracy: 0.9111\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.4387 - accuracy: 0.8439 - val_loss: 0.3475 - val_accuracy: 0.8963\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3981 - accuracy: 0.8471 - val_loss: 0.3586 - val_accuracy: 0.8815\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 17ms/step - loss: 0.4020 - accuracy: 0.8631 - val_loss: 0.3414 - val_accuracy: 0.9111\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3687 - accuracy: 0.8599 - val_loss: 0.3363 - val_accuracy: 0.9037\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3919 - accuracy: 0.8312 - val_loss: 0.3364 - val_accuracy: 0.8963\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.3681 - accuracy: 0.8646 - val_loss: 0.3307 - val_accuracy: 0.9037\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3923 - accuracy: 0.8646 - val_loss: 0.3439 - val_accuracy: 0.8963\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3661 - accuracy: 0.8822 - val_loss: 0.3253 - val_accuracy: 0.9185\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.4383 - accuracy: 0.8376 - val_loss: 0.3593 - val_accuracy: 0.8741\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4112 - accuracy: 0.8471 - val_loss: 0.3393 - val_accuracy: 0.8963\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.4019 - accuracy: 0.8646 - val_loss: 0.3300 - val_accuracy: 0.8963\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.4090 - accuracy: 0.8535 - val_loss: 0.3325 - val_accuracy: 0.9185\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.3743 - accuracy: 0.8599 - val_loss: 0.3320 - val_accuracy: 0.8889\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.4087 - accuracy: 0.8646 - val_loss: 0.3247 - val_accuracy: 0.9037\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.4192 - accuracy: 0.8328 - val_loss: 0.3304 - val_accuracy: 0.8889\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.3690 - accuracy: 0.8567 - val_loss: 0.3334 - val_accuracy: 0.8889\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.3538 - accuracy: 0.8599 - val_loss: 0.3220 - val_accuracy: 0.8889\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.3426 - accuracy: 0.8710 - val_loss: 0.3203 - val_accuracy: 0.8963\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.3773 - accuracy: 0.8455 - val_loss: 0.3183 - val_accuracy: 0.8889\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.3673 - accuracy: 0.8646 - val_loss: 0.3274 - val_accuracy: 0.9037\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.3747 - accuracy: 0.8710 - val_loss: 0.3226 - val_accuracy: 0.8963\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.3690 - accuracy: 0.8583 - val_loss: 0.3240 - val_accuracy: 0.8963\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 24ms/step - loss: 0.4004 - accuracy: 0.8551 - val_loss: 0.3202 - val_accuracy: 0.9111\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.3674 - accuracy: 0.8694 - val_loss: 0.3210 - val_accuracy: 0.9111\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.3210 - accuracy: 0.9111\n",
            "5/5 [==============================] - 0s 8ms/step\n"
          ]
        }
      ],
      "source": [
        "# Model 3: Adding dropout\n",
        "from keras.layers import Dropout\n",
        "model_3 = keras.Sequential([\n",
        "    keras.layers.Input(34),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    keras.layers.Dense(7, activation='softmax')\n",
        "])\n",
        "model_3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_3 = model_3.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_val_scaled, y_val))\n",
        "model_descriptions.append(\"2 layers: 128, 64 neurons. Adam optimizer. Relu activations. Dropout layers.\")\n",
        "training_accuracies.append(history_3.history['accuracy'][-1])\n",
        "validation_loss, validation_accuracy = model_3.evaluate(X_val_scaled, y_val)\n",
        "validation_accuracies.append(validation_accuracy)\n",
        "y_val_pred = model_3.predict(X_val_scaled)\n",
        "y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
        "confusion_matrices.append(confusion_matrix(y_val, y_val_pred_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0tNY7CfbxuD",
        "outputId": "8070cbd6-affa-4088-8b15-016cc6601350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 2s 41ms/step - loss: 1.9526 - accuracy: 0.2675 - val_loss: 1.8914 - val_accuracy: 0.4000\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 1.8453 - accuracy: 0.4029 - val_loss: 1.8327 - val_accuracy: 0.4296\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 1.7817 - accuracy: 0.5510 - val_loss: 1.7986 - val_accuracy: 0.5630\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 1.7401 - accuracy: 0.6226 - val_loss: 1.7712 - val_accuracy: 0.5778\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 1.7035 - accuracy: 0.6401 - val_loss: 1.7452 - val_accuracy: 0.5852\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 1.6671 - accuracy: 0.6401 - val_loss: 1.7183 - val_accuracy: 0.5778\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 1.6297 - accuracy: 0.6369 - val_loss: 1.6870 - val_accuracy: 0.5704\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 1.5889 - accuracy: 0.6290 - val_loss: 1.6511 - val_accuracy: 0.5778\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 1.5479 - accuracy: 0.6338 - val_loss: 1.6169 - val_accuracy: 0.5630\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 1.5070 - accuracy: 0.6306 - val_loss: 1.5823 - val_accuracy: 0.5778\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 1.4660 - accuracy: 0.6354 - val_loss: 1.5456 - val_accuracy: 0.5704\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 1.4260 - accuracy: 0.6338 - val_loss: 1.5116 - val_accuracy: 0.5704\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 1.3868 - accuracy: 0.6354 - val_loss: 1.4765 - val_accuracy: 0.5704\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 1.3484 - accuracy: 0.6385 - val_loss: 1.4447 - val_accuracy: 0.5704\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 1.3113 - accuracy: 0.6385 - val_loss: 1.4086 - val_accuracy: 0.5778\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 1.2749 - accuracy: 0.6385 - val_loss: 1.3750 - val_accuracy: 0.5852\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 1.2407 - accuracy: 0.6401 - val_loss: 1.3398 - val_accuracy: 0.5852\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 1.2073 - accuracy: 0.6417 - val_loss: 1.3073 - val_accuracy: 0.5852\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 1.1756 - accuracy: 0.6417 - val_loss: 1.2744 - val_accuracy: 0.5852\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 1.1454 - accuracy: 0.6417 - val_loss: 1.2443 - val_accuracy: 0.5852\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 1.1163 - accuracy: 0.6433 - val_loss: 1.2159 - val_accuracy: 0.5852\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 1.0879 - accuracy: 0.6433 - val_loss: 1.1902 - val_accuracy: 0.5852\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 1.0624 - accuracy: 0.6433 - val_loss: 1.1615 - val_accuracy: 0.5852\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 1.0363 - accuracy: 0.6433 - val_loss: 1.1352 - val_accuracy: 0.5852\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 1.0129 - accuracy: 0.6433 - val_loss: 1.1094 - val_accuracy: 0.5852\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.9893 - accuracy: 0.6433 - val_loss: 1.0860 - val_accuracy: 0.5926\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.9674 - accuracy: 0.6465 - val_loss: 1.0632 - val_accuracy: 0.5926\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.9460 - accuracy: 0.6465 - val_loss: 1.0435 - val_accuracy: 0.6000\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.9267 - accuracy: 0.6529 - val_loss: 1.0210 - val_accuracy: 0.6000\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.9074 - accuracy: 0.6640 - val_loss: 1.0008 - val_accuracy: 0.6000\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.8888 - accuracy: 0.6672 - val_loss: 0.9832 - val_accuracy: 0.6148\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.8708 - accuracy: 0.6911 - val_loss: 0.9638 - val_accuracy: 0.6370\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.8545 - accuracy: 0.6879 - val_loss: 0.9461 - val_accuracy: 0.6296\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.8394 - accuracy: 0.7006 - val_loss: 0.9317 - val_accuracy: 0.6519\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.8240 - accuracy: 0.7054 - val_loss: 0.9128 - val_accuracy: 0.6593\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.8093 - accuracy: 0.7213 - val_loss: 0.8960 - val_accuracy: 0.6519\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.7949 - accuracy: 0.7245 - val_loss: 0.8800 - val_accuracy: 0.6741\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.7818 - accuracy: 0.7277 - val_loss: 0.8677 - val_accuracy: 0.6667\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.7692 - accuracy: 0.7404 - val_loss: 0.8550 - val_accuracy: 0.6815\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.7563 - accuracy: 0.7484 - val_loss: 0.8421 - val_accuracy: 0.6667\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.7450 - accuracy: 0.7500 - val_loss: 0.8306 - val_accuracy: 0.6667\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.7337 - accuracy: 0.7468 - val_loss: 0.8178 - val_accuracy: 0.6889\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.7217 - accuracy: 0.7596 - val_loss: 0.8089 - val_accuracy: 0.6815\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.7121 - accuracy: 0.7580 - val_loss: 0.7947 - val_accuracy: 0.7111\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.7021 - accuracy: 0.7643 - val_loss: 0.7815 - val_accuracy: 0.6963\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.6919 - accuracy: 0.7675 - val_loss: 0.7738 - val_accuracy: 0.7185\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.6820 - accuracy: 0.7707 - val_loss: 0.7624 - val_accuracy: 0.7185\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.6741 - accuracy: 0.7675 - val_loss: 0.7552 - val_accuracy: 0.7185\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.6643 - accuracy: 0.7755 - val_loss: 0.7437 - val_accuracy: 0.7185\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.6557 - accuracy: 0.7882 - val_loss: 0.7345 - val_accuracy: 0.7259\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.6486 - accuracy: 0.7962 - val_loss: 0.7243 - val_accuracy: 0.7185\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.6404 - accuracy: 0.7962 - val_loss: 0.7167 - val_accuracy: 0.7333\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.6326 - accuracy: 0.7946 - val_loss: 0.7070 - val_accuracy: 0.7407\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.6254 - accuracy: 0.7946 - val_loss: 0.7003 - val_accuracy: 0.7333\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.6173 - accuracy: 0.8010 - val_loss: 0.6934 - val_accuracy: 0.7556\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.6107 - accuracy: 0.8105 - val_loss: 0.6835 - val_accuracy: 0.7481\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.6040 - accuracy: 0.8153 - val_loss: 0.6758 - val_accuracy: 0.7481\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.5987 - accuracy: 0.8169 - val_loss: 0.6734 - val_accuracy: 0.7481\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.5914 - accuracy: 0.8201 - val_loss: 0.6661 - val_accuracy: 0.7630\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.5845 - accuracy: 0.8201 - val_loss: 0.6602 - val_accuracy: 0.7630\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5789 - accuracy: 0.8137 - val_loss: 0.6587 - val_accuracy: 0.7778\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.5745 - accuracy: 0.8201 - val_loss: 0.6458 - val_accuracy: 0.7852\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.5686 - accuracy: 0.8232 - val_loss: 0.6375 - val_accuracy: 0.7852\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.5624 - accuracy: 0.8169 - val_loss: 0.6313 - val_accuracy: 0.7852\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.5572 - accuracy: 0.8248 - val_loss: 0.6299 - val_accuracy: 0.7926\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.5525 - accuracy: 0.8217 - val_loss: 0.6222 - val_accuracy: 0.7704\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5468 - accuracy: 0.8248 - val_loss: 0.6147 - val_accuracy: 0.7778\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.5431 - accuracy: 0.8280 - val_loss: 0.6083 - val_accuracy: 0.7852\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5369 - accuracy: 0.8328 - val_loss: 0.6017 - val_accuracy: 0.8000\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5320 - accuracy: 0.8424 - val_loss: 0.5982 - val_accuracy: 0.7778\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.5303 - accuracy: 0.8439 - val_loss: 0.5969 - val_accuracy: 0.7778\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.5246 - accuracy: 0.8328 - val_loss: 0.5895 - val_accuracy: 0.7926\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5201 - accuracy: 0.8328 - val_loss: 0.5855 - val_accuracy: 0.7926\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.5161 - accuracy: 0.8455 - val_loss: 0.5839 - val_accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5123 - accuracy: 0.8376 - val_loss: 0.5767 - val_accuracy: 0.8000\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.5077 - accuracy: 0.8535 - val_loss: 0.5717 - val_accuracy: 0.7852\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5033 - accuracy: 0.8583 - val_loss: 0.5717 - val_accuracy: 0.8074\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.5018 - accuracy: 0.8424 - val_loss: 0.5686 - val_accuracy: 0.8000\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4980 - accuracy: 0.8439 - val_loss: 0.5633 - val_accuracy: 0.8074\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.4939 - accuracy: 0.8631 - val_loss: 0.5566 - val_accuracy: 0.8000\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.4890 - accuracy: 0.8455 - val_loss: 0.5562 - val_accuracy: 0.7926\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.4862 - accuracy: 0.8471 - val_loss: 0.5515 - val_accuracy: 0.8222\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.4836 - accuracy: 0.8583 - val_loss: 0.5512 - val_accuracy: 0.8000\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.4803 - accuracy: 0.8551 - val_loss: 0.5419 - val_accuracy: 0.8148\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.4763 - accuracy: 0.8535 - val_loss: 0.5435 - val_accuracy: 0.8148\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4745 - accuracy: 0.8535 - val_loss: 0.5346 - val_accuracy: 0.8296\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.4698 - accuracy: 0.8519 - val_loss: 0.5293 - val_accuracy: 0.8074\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4666 - accuracy: 0.8599 - val_loss: 0.5344 - val_accuracy: 0.8296\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.4645 - accuracy: 0.8599 - val_loss: 0.5338 - val_accuracy: 0.8148\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.4637 - accuracy: 0.8519 - val_loss: 0.5258 - val_accuracy: 0.8296\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4583 - accuracy: 0.8535 - val_loss: 0.5174 - val_accuracy: 0.8148\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4572 - accuracy: 0.8631 - val_loss: 0.5125 - val_accuracy: 0.8148\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4553 - accuracy: 0.8662 - val_loss: 0.5215 - val_accuracy: 0.8222\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.4524 - accuracy: 0.8567 - val_loss: 0.5166 - val_accuracy: 0.8296\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4505 - accuracy: 0.8535 - val_loss: 0.5089 - val_accuracy: 0.8222\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4476 - accuracy: 0.8646 - val_loss: 0.5097 - val_accuracy: 0.8296\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.4436 - accuracy: 0.8662 - val_loss: 0.5086 - val_accuracy: 0.8296\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.4420 - accuracy: 0.8599 - val_loss: 0.5033 - val_accuracy: 0.8222\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.4400 - accuracy: 0.8583 - val_loss: 0.5037 - val_accuracy: 0.8074\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.4365 - accuracy: 0.8662 - val_loss: 0.4952 - val_accuracy: 0.8148\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.4952 - accuracy: 0.8148\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        }
      ],
      "source": [
        "# Model 4: Changing optimizer to SGD\n",
        "model_4 = keras.Sequential([\n",
        "    keras.layers.Input(34),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(7, activation='softmax')\n",
        "])\n",
        "model_4.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_4 = model_4.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_val_scaled, y_val))\n",
        "model_descriptions.append(\"2 layers: 128, 64 neurons. SGD optimizer. Relu activations.\")\n",
        "training_accuracies.append(history_4.history['accuracy'][-1])\n",
        "validation_loss, validation_accuracy = model_4.evaluate(X_val_scaled, y_val)\n",
        "validation_accuracies.append(validation_accuracy)\n",
        "y_val_pred = model_4.predict(X_val_scaled)\n",
        "y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
        "confusion_matrices.append(confusion_matrix(y_val, y_val_pred_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLvmdB8BcFtL",
        "outputId": "a7bbbb88-5b47-4a39-9173-0fbb3e01a7a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "20/20 [==============================] - 3s 35ms/step - loss: 1.8211 - accuracy: 0.4283 - val_loss: 1.7206 - val_accuracy: 0.4815\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 1.4972 - accuracy: 0.6131 - val_loss: 1.4477 - val_accuracy: 0.5852\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 1.2311 - accuracy: 0.6401 - val_loss: 1.2176 - val_accuracy: 0.5852\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 1.0349 - accuracy: 0.6449 - val_loss: 1.0359 - val_accuracy: 0.6000\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.8760 - accuracy: 0.6799 - val_loss: 0.8875 - val_accuracy: 0.6296\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.7625 - accuracy: 0.7468 - val_loss: 0.7856 - val_accuracy: 0.7259\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.6788 - accuracy: 0.7978 - val_loss: 0.7099 - val_accuracy: 0.7852\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.6204 - accuracy: 0.7994 - val_loss: 0.6740 - val_accuracy: 0.8148\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5836 - accuracy: 0.8217 - val_loss: 0.6403 - val_accuracy: 0.8222\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.5528 - accuracy: 0.8328 - val_loss: 0.6057 - val_accuracy: 0.8074\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.5277 - accuracy: 0.8360 - val_loss: 0.5594 - val_accuracy: 0.8074\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.5032 - accuracy: 0.8503 - val_loss: 0.5525 - val_accuracy: 0.8296\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.4821 - accuracy: 0.8583 - val_loss: 0.5273 - val_accuracy: 0.8444\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.4707 - accuracy: 0.8615 - val_loss: 0.5444 - val_accuracy: 0.8222\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.4589 - accuracy: 0.8631 - val_loss: 0.5078 - val_accuracy: 0.8370\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.4416 - accuracy: 0.8662 - val_loss: 0.5151 - val_accuracy: 0.8519\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4441 - accuracy: 0.8535 - val_loss: 0.4733 - val_accuracy: 0.8593\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4321 - accuracy: 0.8726 - val_loss: 0.5166 - val_accuracy: 0.8667\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4278 - accuracy: 0.8678 - val_loss: 0.4824 - val_accuracy: 0.8519\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.4148 - accuracy: 0.8758 - val_loss: 0.4634 - val_accuracy: 0.8741\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.4035 - accuracy: 0.8742 - val_loss: 0.4676 - val_accuracy: 0.8741\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.4076 - accuracy: 0.8742 - val_loss: 0.4392 - val_accuracy: 0.9185\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3928 - accuracy: 0.8806 - val_loss: 0.4289 - val_accuracy: 0.8889\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.3841 - accuracy: 0.8838 - val_loss: 0.4439 - val_accuracy: 0.8889\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.3766 - accuracy: 0.8917 - val_loss: 0.4311 - val_accuracy: 0.9111\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3747 - accuracy: 0.8901 - val_loss: 0.4191 - val_accuracy: 0.8963\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3697 - accuracy: 0.8869 - val_loss: 0.4149 - val_accuracy: 0.8963\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3682 - accuracy: 0.8917 - val_loss: 0.4116 - val_accuracy: 0.9185\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3617 - accuracy: 0.8901 - val_loss: 0.4087 - val_accuracy: 0.9185\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.3593 - accuracy: 0.8885 - val_loss: 0.4455 - val_accuracy: 0.8296\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3629 - accuracy: 0.9076 - val_loss: 0.4117 - val_accuracy: 0.8815\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.3487 - accuracy: 0.8917 - val_loss: 0.3878 - val_accuracy: 0.9111\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.3421 - accuracy: 0.9013 - val_loss: 0.3808 - val_accuracy: 0.9037\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.3384 - accuracy: 0.9045 - val_loss: 0.4029 - val_accuracy: 0.9037\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.3370 - accuracy: 0.9061 - val_loss: 0.3690 - val_accuracy: 0.8963\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3396 - accuracy: 0.8981 - val_loss: 0.3793 - val_accuracy: 0.8741\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.3358 - accuracy: 0.8949 - val_loss: 0.3957 - val_accuracy: 0.8963\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3272 - accuracy: 0.9029 - val_loss: 0.3564 - val_accuracy: 0.9185\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3198 - accuracy: 0.9108 - val_loss: 0.3646 - val_accuracy: 0.9185\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.3194 - accuracy: 0.9061 - val_loss: 0.3495 - val_accuracy: 0.9259\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3126 - accuracy: 0.9124 - val_loss: 0.3575 - val_accuracy: 0.9185\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.3121 - accuracy: 0.9140 - val_loss: 0.3634 - val_accuracy: 0.9037\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.3106 - accuracy: 0.9156 - val_loss: 0.3344 - val_accuracy: 0.9185\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.3116 - accuracy: 0.9124 - val_loss: 0.3395 - val_accuracy: 0.9185\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.3163 - accuracy: 0.9124 - val_loss: 0.3396 - val_accuracy: 0.9259\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.3022 - accuracy: 0.9076 - val_loss: 0.3326 - val_accuracy: 0.9259\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 11ms/step - loss: 0.2915 - accuracy: 0.9188 - val_loss: 0.3259 - val_accuracy: 0.9185\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2953 - accuracy: 0.9156 - val_loss: 0.3262 - val_accuracy: 0.9185\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2982 - accuracy: 0.9029 - val_loss: 0.3247 - val_accuracy: 0.9259\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2923 - accuracy: 0.9108 - val_loss: 0.3294 - val_accuracy: 0.8889\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2894 - accuracy: 0.9220 - val_loss: 0.3164 - val_accuracy: 0.9259\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2829 - accuracy: 0.9252 - val_loss: 0.3139 - val_accuracy: 0.8963\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 20ms/step - loss: 0.2825 - accuracy: 0.9220 - val_loss: 0.3333 - val_accuracy: 0.9111\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2787 - accuracy: 0.9204 - val_loss: 0.3262 - val_accuracy: 0.9037\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2814 - accuracy: 0.9140 - val_loss: 0.3465 - val_accuracy: 0.9037\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.2699 - accuracy: 0.9252 - val_loss: 0.3032 - val_accuracy: 0.9111\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.2709 - accuracy: 0.9188 - val_loss: 0.3121 - val_accuracy: 0.9111\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2639 - accuracy: 0.9299 - val_loss: 0.3097 - val_accuracy: 0.9111\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2728 - accuracy: 0.9172 - val_loss: 0.3169 - val_accuracy: 0.8963\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 12ms/step - loss: 0.2868 - accuracy: 0.8981 - val_loss: 0.3393 - val_accuracy: 0.9037\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2853 - accuracy: 0.9124 - val_loss: 0.3094 - val_accuracy: 0.9037\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2611 - accuracy: 0.9252 - val_loss: 0.3138 - val_accuracy: 0.8741\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2689 - accuracy: 0.9283 - val_loss: 0.3153 - val_accuracy: 0.8815\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2674 - accuracy: 0.9347 - val_loss: 0.3068 - val_accuracy: 0.9037\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2587 - accuracy: 0.9252 - val_loss: 0.3198 - val_accuracy: 0.8889\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2514 - accuracy: 0.9331 - val_loss: 0.3053 - val_accuracy: 0.9185\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2611 - accuracy: 0.9204 - val_loss: 0.3073 - val_accuracy: 0.8889\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2661 - accuracy: 0.9236 - val_loss: 0.3041 - val_accuracy: 0.9111\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2514 - accuracy: 0.9331 - val_loss: 0.3083 - val_accuracy: 0.8963\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2529 - accuracy: 0.9268 - val_loss: 0.3021 - val_accuracy: 0.9111\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2487 - accuracy: 0.9315 - val_loss: 0.2940 - val_accuracy: 0.8963\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2569 - accuracy: 0.9252 - val_loss: 0.3342 - val_accuracy: 0.8963\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2435 - accuracy: 0.9331 - val_loss: 0.3224 - val_accuracy: 0.8889\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2496 - accuracy: 0.9283 - val_loss: 0.3228 - val_accuracy: 0.8815\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2449 - accuracy: 0.9283 - val_loss: 0.2925 - val_accuracy: 0.9185\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2512 - accuracy: 0.9379 - val_loss: 0.3191 - val_accuracy: 0.9111\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2419 - accuracy: 0.9379 - val_loss: 0.2948 - val_accuracy: 0.9037\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2402 - accuracy: 0.9315 - val_loss: 0.2868 - val_accuracy: 0.9111\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2408 - accuracy: 0.9268 - val_loss: 0.3044 - val_accuracy: 0.9037\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2301 - accuracy: 0.9347 - val_loss: 0.3011 - val_accuracy: 0.9185\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2386 - accuracy: 0.9363 - val_loss: 0.3153 - val_accuracy: 0.9037\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2368 - accuracy: 0.9315 - val_loss: 0.2840 - val_accuracy: 0.9037\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2359 - accuracy: 0.9315 - val_loss: 0.2734 - val_accuracy: 0.9111\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2411 - accuracy: 0.9299 - val_loss: 0.2840 - val_accuracy: 0.8815\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2326 - accuracy: 0.9315 - val_loss: 0.2869 - val_accuracy: 0.8963\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2339 - accuracy: 0.9347 - val_loss: 0.2781 - val_accuracy: 0.9037\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2212 - accuracy: 0.9443 - val_loss: 0.3038 - val_accuracy: 0.8963\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2255 - accuracy: 0.9347 - val_loss: 0.2801 - val_accuracy: 0.9111\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.2299 - accuracy: 0.9315 - val_loss: 0.2774 - val_accuracy: 0.8963\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2184 - accuracy: 0.9427 - val_loss: 0.2844 - val_accuracy: 0.8963\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 10ms/step - loss: 0.2225 - accuracy: 0.9347 - val_loss: 0.2669 - val_accuracy: 0.8963\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2190 - accuracy: 0.9315 - val_loss: 0.3426 - val_accuracy: 0.8741\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2233 - accuracy: 0.9411 - val_loss: 0.2914 - val_accuracy: 0.9037\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.2210 - accuracy: 0.9427 - val_loss: 0.2724 - val_accuracy: 0.9037\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.2150 - accuracy: 0.9363 - val_loss: 0.2865 - val_accuracy: 0.8889\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.2225 - accuracy: 0.9363 - val_loss: 0.2671 - val_accuracy: 0.8889\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.2148 - accuracy: 0.9363 - val_loss: 0.2915 - val_accuracy: 0.9111\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2070 - accuracy: 0.9427 - val_loss: 0.2747 - val_accuracy: 0.8889\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.2090 - accuracy: 0.9363 - val_loss: 0.2810 - val_accuracy: 0.8963\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.2092 - accuracy: 0.9443 - val_loss: 0.2842 - val_accuracy: 0.9111\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.2842 - accuracy: 0.9111\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# Model 5: Using L1 regularization\n",
        "from keras.regularizers import l1\n",
        "model_5 = keras.Sequential([\n",
        "    keras.layers.Input(34),\n",
        "    keras.layers.Dense(128, activation='relu', activity_regularizer=l1(0.001)),\n",
        "    keras.layers.Dense(64, activation='relu', activity_regularizer=l1(0.001)),\n",
        "    keras.layers.Dense(7, activation='softmax')\n",
        "])\n",
        "model_5.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_5 = model_5.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_val_scaled, y_val))\n",
        "model_descriptions.append(\"2 layers: 128, 64 neurons. Adam optimizer. Relu activations. L1 regularization.\")\n",
        "training_accuracies.append(history_5.history['accuracy'][-1])\n",
        "validation_loss, validation_accuracy = model_5.evaluate(X_val_scaled, y_val)\n",
        "validation_accuracies.append(validation_accuracy)\n",
        "y_val_pred = model_5.predict(X_val_scaled)\n",
        "y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
        "confusion_matrices.append(confusion_matrix(y_val, y_val_pred_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB1rXo7rV38L",
        "outputId": "6b8f36ed-980b-4595-91a6-b66c6284ed78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Description: 3 layers: 256, 128, 64 neurons. Adam optimizer. Relu activations.\n",
            "Training Accuracy: 0.9554139971733093\n",
            "Validation Accuracy: 0.8814814686775208\n",
            "Confusion Matrix:\n",
            "[[12  0  0  0  1  0  0]\n",
            " [ 0 13  3  0  0  0  5]\n",
            " [ 0  1 23  0  0  0  1]\n",
            " [ 2  0  0  6  0  0  0]\n",
            " [ 0  0  0  0 27  0  1]\n",
            " [ 0  0  0  0  0 26  0]\n",
            " [ 1  1  0  0  0  0 12]]\n",
            "--------------------------------------------------\n",
            "Model Description: 2 layers: 128, 64 neurons. Adam optimizer. Tanh activations.\n",
            "Training Accuracy: 0.9474522471427917\n",
            "Validation Accuracy: 0.9037036895751953\n",
            "Confusion Matrix:\n",
            "[[12  0  0  0  1  0  0]\n",
            " [ 0 14  2  0  0  0  5]\n",
            " [ 0  3 21  0  0  0  1]\n",
            " [ 0  0  0  8  0  0  0]\n",
            " [ 0  0  0  0 28  0  0]\n",
            " [ 0  0  0  0  0 26  0]\n",
            " [ 0  0  0  0  1  0 13]]\n",
            "--------------------------------------------------\n",
            "Model Description: 2 layers: 128, 64 neurons. Adam optimizer. Relu activations. Dropout layers.\n",
            "Training Accuracy: 0.8694267272949219\n",
            "Validation Accuracy: 0.9111111164093018\n",
            "Confusion Matrix:\n",
            "[[12  0  0  0  1  0  0]\n",
            " [ 0 14  3  0  0  0  4]\n",
            " [ 0  1 24  0  0  0  0]\n",
            " [ 0  0  0  8  0  0  0]\n",
            " [ 0  0  0  0 27  0  1]\n",
            " [ 0  0  0  0  0 26  0]\n",
            " [ 0  0  0  0  1  1 12]]\n",
            "--------------------------------------------------\n",
            "Model Description: 2 layers: 128, 64 neurons. SGD optimizer. Relu activations.\n",
            "Training Accuracy: 0.8662420511245728\n",
            "Validation Accuracy: 0.8148148059844971\n",
            "Confusion Matrix:\n",
            "[[ 6  0  0  5  2  0  0]\n",
            " [ 0 13  3  0  2  0  3]\n",
            " [ 0  1 24  0  0  0  0]\n",
            " [ 2  0  0  6  0  0  0]\n",
            " [ 0  0  0  0 27  0  1]\n",
            " [ 0  0  0  0  0 26  0]\n",
            " [ 0  2  0  1  3  0  8]]\n",
            "--------------------------------------------------\n",
            "Model Description: 2 layers: 128, 64 neurons. Adam optimizer. Relu activations. L1 regularization.\n",
            "Training Accuracy: 0.9442675113677979\n",
            "Validation Accuracy: 0.9111111164093018\n",
            "Confusion Matrix:\n",
            "[[12  0  0  0  1  0  0]\n",
            " [ 0 13  3  0  0  0  5]\n",
            " [ 0  0 24  0  0  0  1]\n",
            " [ 0  0  0  8  0  0  0]\n",
            " [ 0  0  0  0 27  0  1]\n",
            " [ 0  0  0  0  0 26  0]\n",
            " [ 1  0  0  0  0  0 13]]\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print results\n",
        "for i in range(5):\n",
        "    print(\"Model Description:\", model_descriptions[i])\n",
        "    print(\"Training Accuracy:\", training_accuracies[i])\n",
        "    print(\"Validation Accuracy:\", validation_accuracies[i])\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrices[i])\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0yZPs7vTMeL"
      },
      "source": [
        "# Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tahvk7THT4m7"
      },
      "source": [
        "# 3A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU-Tr763gtuL"
      },
      "source": [
        "We bleive that **Model_2 : 2 layers: 128, 64 neurons. Adam optimizer. Tanh activations.** is the best model; since it has the highest Training Accuracy: 0.9522\n",
        "and good Validation Accuracy: 0.9185\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSE0Tf78UAFC"
      },
      "source": [
        "# 3B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uUANjRqUBg6",
        "outputId": "bd1805b0-c254-4327-fd35-3a9ed52df9d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1871 - accuracy: 0.9333\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 3ms/step\n",
            "Testing Accuracy: 0.9333333373069763\n",
            "Confusion Matrix for Testing Data:\n",
            "[[ 6  0  0  0  0  0  0]\n",
            " [ 0 12  2  0  0  0  2]\n",
            " [ 0  2 29  0  0  0  0]\n",
            " [ 1  0  0 13  0  0  0]\n",
            " [ 0  0  0  0 23  0  2]\n",
            " [ 0  0  0  0  0 29  0]\n",
            " [ 0  0  0  0  0  0 14]]\n"
          ]
        }
      ],
      "source": [
        "# Evaluating model on testing dataset\n",
        "test_loss, test_accuracy = model_2.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "# Predict on the testing dataset\n",
        "y_test_pred = model_2.predict(X_test_scaled)\n",
        "y_test_pred_labels = np.argmax(y_test_pred, axis=1)\n",
        "\n",
        "# Confusion Matrix for the testing dataset\n",
        "test_confusion_matrix = confusion_matrix(y_test, y_test_pred_labels)\n",
        "\n",
        "# Printing the results.\n",
        "print(f\"Testing Accuracy: {test_accuracy}\")\n",
        "print(\"Confusion Matrix for Testing Data:\")\n",
        "print(test_confusion_matrix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIqCF49gUCnq"
      },
      "source": [
        "# 3C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the current use case, model 3 and model 5 stand out as strong performers due to their combination of solid training accuracy and high validation accuracy. They also demonstrate balanced precision and recall across classes in their respective confusion matrices. Model 2 is also a good choice with a balanced trade-off between accuracy and generalization."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
